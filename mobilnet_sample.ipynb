{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 1.12.1\n",
      "torchvision = 0.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def memory_check():\n",
    "    print(f\"  Allocated: {round(torch.cuda.memory_allocated()/1024**3,2)} GB\")\n",
    "    print(f\"  Cached:    {round(torch.cuda.memory_reserved()/1024**3,2)} GB\\n\")\n",
    "\n",
    "print(f\"torch = {torch.__version__}\")\n",
    "print(f\"torchvision = {torchvision.__version__}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ImageNet(validation 6G) Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "def ImageNet_DataLoader(split_num = [0.08,0.02,0.9]):\n",
    "    if not os.path.exists(\"./data/ImageNet/meta.bin\"):\n",
    "        print(\"Meta data download\")\n",
    "        wget.download(url=\"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\", out=\"./data/ImageNet\")\n",
    "    # if not os.path.exists(\"./data/ImageNet/ILSVRC2012_devkit_t3.tar.gz\"):\n",
    "    #     print(\"Toolkit t3 Download\")\n",
    "    #     toolkit_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t3.tar.gz\"\n",
    "    #     wget.download(url= toolkit_url,out=\"./data/ImageNet\")\n",
    "    if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_val.tar\"):\n",
    "        print(\"Download val data\")\n",
    "        val_url  = 'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar'\n",
    "        wget.download(url=val_url, out=\"./data/ImageNet\")\n",
    "\n",
    "    # if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_train_t3.tar\"):\n",
    "    #     print(\"Download train t3 data\")\n",
    "    #     train_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train_t3.tar\"\n",
    "    #     wget.download(url=train_url,out=\"./data/ImageNet\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    print(os.getcwd())\n",
    "    dataset = torchvision.datasets.ImageNet(root=\"./data/ImageNet\",split=\"val\", transform = train_transform)\n",
    "    Train_dataset, Test_dataset,_ = torch.utils.data.random_split(dataset, split_num)\n",
    "    print(f\"Train data set = {len(Train_dataset)}, Test = {len(Test_dataset)}\")\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(Train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(Test_dataset)\n",
    "\n",
    "    Train_loader = torch.utils.data.DataLoader(dataset=Train_dataset, batch_size= 32, sampler = train_sampler)\n",
    "    Test_loader = torch.utils.data.DataLoader(dataset=Test_dataset, batch_size =32, sampler = test_sampler)\n",
    "    return Train_loader, Test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cifar10_Dataloader():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding = 4),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "    print(f\"Train data set = {len(train_dataset)}, Test = {len(test_dataset)}\")\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=128,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset, batch_size=128,\n",
    "        sampler=test_sampler)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Fuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluating(model, test_loader, device, criterion=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(iter(test_loader)):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "        # statistics\n",
    "        running_loss += loss * labels.size(0)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = 100 * running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(model, train_loader, test_loader, device, optimizer, scheduler, epochs=100,model_name=\"test\"):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    print(\"Before Training\")\n",
    "    torch.cuda.memory_reserved()\n",
    "    memory_check()\n",
    "    count = 0\n",
    "    best_loss = np.Inf\n",
    "    # Training\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in tqdm(iter(train_loader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    " \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # statistics\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * running_corrects / len(train_loader.dataset) \n",
    "\n",
    "        # Evaluation\n",
    "        val_loss, val_acc = Evaluating(model,test_loader,device=device,criterion=criterion)\n",
    "        print(f\"--------{epoch+1}----------\")\n",
    "        print(f\"Train {train_loss:.4f} Loss, {train_accuracy:.2f} Acc\")\n",
    "        print(f\"Validation {val_loss:.4f} Loss, {val_acc:.2f} Acc\")\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            count = 0\n",
    "            torch.save(model.state_dict(), f\"./models/{model_name}.pt\")\n",
    "        else:\n",
    "            count +=1\n",
    "            if count > 10:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(f\"./models/{model_name}.pt\")) \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer fusion Check\n",
    "conv, bn, relu를 하나의 layer로 만들어 각각의 layer를 읽어오는 연산을 줄이는 과정   \n",
    "folding과는 다른 경량화 기법   \n",
    "Fusion 된 layer는 identity로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eq_check(model1, model2, device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model1(x).detach().cpu().numpy()\n",
    "        y2 = model2(x).detach().cpu().numpy()\n",
    "        # 배열이 허용 오차범위 abs(a - b) <= (atol + rtol * absolute(b)) 이내면 True\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test fail\")\n",
    "            return False\n",
    "    print(\"Two models equal\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(model, device, input_size = (1,3,256,256),num_tests=100,):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(num_tests):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "    aver_time = total_time / num_tests\n",
    "    return total_time, aver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnReLUModel(\n",
      "  (conv): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "ConvBnReLUModel(\n",
      "  (conv): ConvReLU2d(\n",
      "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn): Identity()\n",
      "  (relu): Identity()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "-- Equal Test --\n",
      "Two models equal\n",
      "-- Infer Time Test --\n",
      "origin model infer time 0.055s\n",
      "fusion model infer time 0.041s\n"
     ]
    }
   ],
   "source": [
    "class ConvBnReLUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBnReLUModel,self).__init__()\n",
    "        self.conv = nn.Conv2d(3,5,3,bias=True).to(dtype=torch.float)\n",
    "        self.bn = nn.BatchNorm2d(5).to(dtype=torch.float)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "model = ConvBnReLUModel().to(device=torch.device(\"cpu:0\"))\n",
    "model.eval()\n",
    "print(model)\n",
    "# for p in model.named_parameters():\n",
    "#     print(p)\n",
    "#     print()\n",
    "# \"fbgemm\" for server , \"qnnpack\" for mobile \n",
    "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# # torch.quantization.fuse_module or myModel.fuse_model()\n",
    "fuse_model = torch.ao.quantization.fuse_modules(model,[['conv','bn','relu']], inplace=False)\n",
    "# fuse_model = model.fuse_model()\n",
    "print(fuse_model)\n",
    "\n",
    "print(f\"-- Equal Test --\")\n",
    "model_eq_check(model, fuse_model, device=torch.device(\"cpu:0\"))\n",
    "\n",
    "\n",
    "print(f\"-- Infer Time Test --\")\n",
    "ori_cpu_time,_ = time_test(model,torch.device(\"cpu\"))\n",
    "fus_cpu_time,_ = time_test(fuse_model,torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"origin model infer time {ori_cpu_time:.3f}s\")\n",
    "print(f\"fusion model infer time {fus_cpu_time:.3f}s\")\n",
    "del model\n",
    "del fuse_model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         QuantStub-1            [-1, 3, 32, 32]               0\n",
      "            Conv2d-2           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-3           [-1, 32, 16, 16]              64\n",
      "              ReLU-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-9           [-1, 16, 16, 16]              32\n",
      "QuantizableInvertedResidual-10           [-1, 16, 16, 16]               0\n",
      "           Conv2d-11           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-12           [-1, 96, 16, 16]             192\n",
      "             ReLU-13           [-1, 96, 16, 16]               0\n",
      "           Conv2d-14           [-1, 96, 16, 16]             864\n",
      "      BatchNorm2d-15           [-1, 96, 16, 16]             192\n",
      "             ReLU-16           [-1, 96, 16, 16]               0\n",
      "           Conv2d-17           [-1, 24, 16, 16]           2,304\n",
      "      BatchNorm2d-18           [-1, 24, 16, 16]              48\n",
      "QuantizableInvertedResidual-19           [-1, 24, 16, 16]               0\n",
      "           Conv2d-20          [-1, 144, 16, 16]           3,456\n",
      "      BatchNorm2d-21          [-1, 144, 16, 16]             288\n",
      "             ReLU-22          [-1, 144, 16, 16]               0\n",
      "           Conv2d-23          [-1, 144, 16, 16]           1,296\n",
      "      BatchNorm2d-24          [-1, 144, 16, 16]             288\n",
      "             ReLU-25          [-1, 144, 16, 16]               0\n",
      "           Conv2d-26           [-1, 24, 16, 16]           3,456\n",
      "      BatchNorm2d-27           [-1, 24, 16, 16]              48\n",
      "         Identity-28           [-1, 24, 16, 16]               0\n",
      "QuantizableInvertedResidual-29           [-1, 24, 16, 16]               0\n",
      "           Conv2d-30          [-1, 144, 16, 16]           3,456\n",
      "      BatchNorm2d-31          [-1, 144, 16, 16]             288\n",
      "             ReLU-32          [-1, 144, 16, 16]               0\n",
      "           Conv2d-33            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-34            [-1, 144, 8, 8]             288\n",
      "             ReLU-35            [-1, 144, 8, 8]               0\n",
      "           Conv2d-36             [-1, 32, 8, 8]           4,608\n",
      "      BatchNorm2d-37             [-1, 32, 8, 8]              64\n",
      "QuantizableInvertedResidual-38             [-1, 32, 8, 8]               0\n",
      "           Conv2d-39            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-40            [-1, 192, 8, 8]             384\n",
      "             ReLU-41            [-1, 192, 8, 8]               0\n",
      "           Conv2d-42            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-43            [-1, 192, 8, 8]             384\n",
      "             ReLU-44            [-1, 192, 8, 8]               0\n",
      "           Conv2d-45             [-1, 32, 8, 8]           6,144\n",
      "      BatchNorm2d-46             [-1, 32, 8, 8]              64\n",
      "         Identity-47             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-48             [-1, 32, 8, 8]               0\n",
      "           Conv2d-49            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-50            [-1, 192, 8, 8]             384\n",
      "             ReLU-51            [-1, 192, 8, 8]               0\n",
      "           Conv2d-52            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-53            [-1, 192, 8, 8]             384\n",
      "             ReLU-54            [-1, 192, 8, 8]               0\n",
      "           Conv2d-55             [-1, 32, 8, 8]           6,144\n",
      "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
      "         Identity-57             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-58             [-1, 32, 8, 8]               0\n",
      "           Conv2d-59            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-60            [-1, 192, 8, 8]             384\n",
      "             ReLU-61            [-1, 192, 8, 8]               0\n",
      "           Conv2d-62            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-63            [-1, 192, 4, 4]             384\n",
      "             ReLU-64            [-1, 192, 4, 4]               0\n",
      "           Conv2d-65             [-1, 64, 4, 4]          12,288\n",
      "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
      "QuantizableInvertedResidual-67             [-1, 64, 4, 4]               0\n",
      "           Conv2d-68            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-69            [-1, 384, 4, 4]             768\n",
      "             ReLU-70            [-1, 384, 4, 4]               0\n",
      "           Conv2d-71            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-72            [-1, 384, 4, 4]             768\n",
      "             ReLU-73            [-1, 384, 4, 4]               0\n",
      "           Conv2d-74             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-75             [-1, 64, 4, 4]             128\n",
      "         Identity-76             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-77             [-1, 64, 4, 4]               0\n",
      "           Conv2d-78            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-79            [-1, 384, 4, 4]             768\n",
      "             ReLU-80            [-1, 384, 4, 4]               0\n",
      "           Conv2d-81            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-82            [-1, 384, 4, 4]             768\n",
      "             ReLU-83            [-1, 384, 4, 4]               0\n",
      "           Conv2d-84             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-85             [-1, 64, 4, 4]             128\n",
      "         Identity-86             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-87             [-1, 64, 4, 4]               0\n",
      "           Conv2d-88            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-89            [-1, 384, 4, 4]             768\n",
      "             ReLU-90            [-1, 384, 4, 4]               0\n",
      "           Conv2d-91            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-92            [-1, 384, 4, 4]             768\n",
      "             ReLU-93            [-1, 384, 4, 4]               0\n",
      "           Conv2d-94             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-95             [-1, 64, 4, 4]             128\n",
      "         Identity-96             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-97             [-1, 64, 4, 4]               0\n",
      "           Conv2d-98            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-99            [-1, 384, 4, 4]             768\n",
      "            ReLU-100            [-1, 384, 4, 4]               0\n",
      "          Conv2d-101            [-1, 384, 4, 4]           3,456\n",
      "     BatchNorm2d-102            [-1, 384, 4, 4]             768\n",
      "            ReLU-103            [-1, 384, 4, 4]               0\n",
      "          Conv2d-104             [-1, 96, 4, 4]          36,864\n",
      "     BatchNorm2d-105             [-1, 96, 4, 4]             192\n",
      "QuantizableInvertedResidual-106             [-1, 96, 4, 4]               0\n",
      "          Conv2d-107            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-108            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-109            [-1, 576, 4, 4]               0\n",
      "          Conv2d-110            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-111            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-112            [-1, 576, 4, 4]               0\n",
      "          Conv2d-113             [-1, 96, 4, 4]          55,296\n",
      "     BatchNorm2d-114             [-1, 96, 4, 4]             192\n",
      "        Identity-115             [-1, 96, 4, 4]               0\n",
      "QuantizableInvertedResidual-116             [-1, 96, 4, 4]               0\n",
      "          Conv2d-117            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-118            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-119            [-1, 576, 4, 4]               0\n",
      "          Conv2d-120            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-121            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-122            [-1, 576, 4, 4]               0\n",
      "          Conv2d-123             [-1, 96, 4, 4]          55,296\n",
      "     BatchNorm2d-124             [-1, 96, 4, 4]             192\n",
      "        Identity-125             [-1, 96, 4, 4]               0\n",
      "QuantizableInvertedResidual-126             [-1, 96, 4, 4]               0\n",
      "          Conv2d-127            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-128            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-129            [-1, 576, 4, 4]               0\n",
      "          Conv2d-130            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-131            [-1, 576, 2, 2]           1,152\n",
      "            ReLU-132            [-1, 576, 2, 2]               0\n",
      "          Conv2d-133            [-1, 160, 2, 2]          92,160\n",
      "     BatchNorm2d-134            [-1, 160, 2, 2]             320\n",
      "QuantizableInvertedResidual-135            [-1, 160, 2, 2]               0\n",
      "          Conv2d-136            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-138            [-1, 960, 2, 2]               0\n",
      "          Conv2d-139            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-141            [-1, 960, 2, 2]               0\n",
      "          Conv2d-142            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 2, 2]             320\n",
      "        Identity-144            [-1, 160, 2, 2]               0\n",
      "QuantizableInvertedResidual-145            [-1, 160, 2, 2]               0\n",
      "          Conv2d-146            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-147            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-148            [-1, 960, 2, 2]               0\n",
      "          Conv2d-149            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-150            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-151            [-1, 960, 2, 2]               0\n",
      "          Conv2d-152            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-153            [-1, 160, 2, 2]             320\n",
      "        Identity-154            [-1, 160, 2, 2]               0\n",
      "QuantizableInvertedResidual-155            [-1, 160, 2, 2]               0\n",
      "          Conv2d-156            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-157            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-158            [-1, 960, 2, 2]               0\n",
      "          Conv2d-159            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-160            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-161            [-1, 960, 2, 2]               0\n",
      "          Conv2d-162            [-1, 320, 2, 2]         307,200\n",
      "     BatchNorm2d-163            [-1, 320, 2, 2]             640\n",
      "QuantizableInvertedResidual-164            [-1, 320, 2, 2]               0\n",
      "          Conv2d-165           [-1, 1280, 2, 2]         409,600\n",
      "     BatchNorm2d-166           [-1, 1280, 2, 2]           2,560\n",
      "            ReLU-167           [-1, 1280, 2, 2]               0\n",
      "         Dropout-168                 [-1, 1280]               0\n",
      "          Linear-169                 [-1, 1000]       1,281,000\n",
      "         Dropout-170                 [-1, 1000]               0\n",
      "          Linear-171                   [-1, 10]          10,010\n",
      "     DeQuantStub-172                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 3,514,882\n",
      "Trainable params: 3,514,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.57\n",
      "Params size (MB): 13.41\n",
      "Estimated Total Size (MB): 22.99\n",
      "----------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train data set = 50000, Test = 10000\n"
     ]
    }
   ],
   "source": [
    "# gpu,cpu device 선언\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# model 가져오기\n",
    "from models import mobilenet_v2, MobileNet_V2_Weights,quat_mobilenet_v2\n",
    "model = quat_mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1,activation_layer=nn.ReLU)\n",
    "model.classifier.append(nn.Dropout(0.2))\n",
    "model.classifier.append(nn.Linear(1000, 10))\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model,(3,32,32), device='cpu') \n",
    "\n",
    "# Move the model to CPU since static quantization does not support CUDA currently.\n",
    "# ImageNet Data \n",
    "Train_loader, Test_loader = Cifar10_Dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "1 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "2 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "3 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "4 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "5 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "6 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "7 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "8 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "9 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "10 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "11 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "12 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "13 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "14 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "15 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "16 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "17 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "18 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "19 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "20 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "21 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(Train_loader):\n",
    "\n",
    "    img = data[0].to(gpu_device)\n",
    "    label = data[1].to(gpu_device)\n",
    "    print(f\"{i} data size = {img.size()}, label size = {label.size()}\")\n",
    "    memory_check()\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:05<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model acc : 90.06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizableMobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (7): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (11): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (14): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (15): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (16): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (17): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_train=False\n",
    "if need_train:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,90], gamma=0.5)\n",
    "\n",
    "    model = Training(model,train_loader=Train_loader,test_loader=Test_loader,device=gpu_device,optimizer=optimizer,scheduler=scheduler,epochs=20,\n",
    "    model_name = \"q_mobilenetv2_cifar10\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"./models/q_mobilenetv2_cifar10.pt\"))\n",
    "    _,pre_acc = Evaluating(model,Test_loader,cpu_device)\n",
    "    print(f\"pretrained model acc : {pre_acc:.2f} %\")\n",
    "    # QAT가 적용된 floating point 모델을 quantized int model로 변환\n",
    "model.to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizableMobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (11): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (14): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (17): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Equal Test between origin and fused\n",
      "Two models equal\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델을 CPU상태로 두고 eval로 layer fusion\n",
    "model.eval()\n",
    "print(model)\n",
    "# Layer fusion\n",
    "# fused_model = torch.quantization.fuse_modules(model,[[\"conv1\",\"bn1\",\"relu\"]])\n",
    "\n",
    "# for module_name, module in fused_model.named_children():\n",
    "#     if \"layer\" in module_name:\n",
    "#         # basic_block 의 conv1, bn1, relu, conv2, bn2 를 fusion\n",
    "#         for basic_block_name, basic_block in module.named_children():\n",
    "#             torch.ao.quantization.fuse_modules(basic_block,[[\"conv1\",\"bn1\",\"relu\"],[\"conv2\",\"bn2\"]],inplace=True)\n",
    "#             # basic_block안의 downsampling block의 Conv2d Batchnorm2D fusion\n",
    "#             for sub_block_name, sub_block in basic_block.named_children():\n",
    "#                 if sub_block_name == \"downsample\":\n",
    "#                     torch.ao.quantization.fuse_modules(sub_block,[[\"0\",\"1\"]], inplace=True)\n",
    "# print(fused_model)\n",
    "fused_model = copy.deepcopy(model)\n",
    "fused_model.fuse_model()\n",
    "# Equal Test\n",
    "print(f\"Equal Test between origin and fused\")\n",
    "print(model_eq_check(model,fused_model,device=cpu_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "    print(\"calibrating ...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 18.92it/s]\n",
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization acc : 90.06 %\n",
      "calibrating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:10<00:00,  7.83it/s]\n",
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1135: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "100%|██████████| 79/79 [00:04<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post int8_model acc :85.65 %\n",
      "<class 'models.mobilenetv2.QuantizableMobileNetV2'>\n"
     ]
    }
   ],
   "source": [
    "from models.mobilenetv2 import quat_mobilenet_v2\n",
    "new = False\n",
    "if new:\n",
    "    quat_model = quat_mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1,activation_layer=nn.ReLU)\n",
    "    print(quat_model)\n",
    "    quat_model.classifier.append(nn.Dropout(0.2))\n",
    "    quat_model.classifier.append(nn.Linear(1000, 10))\n",
    "    quat_model.fuse_model()\n",
    "else:\n",
    "    pre_model = copy.deepcopy(fused_model)\n",
    "    _,acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "    print(f\"Before quantization acc : {acc:.2f} %\")\n",
    "    pre_model.eval()\n",
    "    pre_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    torch.quantization.prepare(pre_model,inplace=True)\n",
    "    calibrate_model(pre_model, Test_loader)\n",
    "    pre_model = torch.quantization.convert(pre_model,inplace=True)\n",
    "    torch.jit.save(torch.jit.script(pre_model),\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "    pre_model = torch.jit.load(\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "    _,int8_acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "    print(f\"post int8_model acc :{int8_acc:.2f} %\")\n",
    "    \n",
    "    quat_model = fused_model\n",
    "# qconfig(\"fbgemm\") 은 server 용 \"qnnpack\"은 mobile용 [\"fbgemm\", \"x86\", \"qnnpack\", \"onednn\"]\n",
    "\n",
    "# QAT를 하기위해 quantization 모델 준비\n",
    "quat_model.train()\n",
    "quat_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "print(quat_model.qconfig)\n",
    "quat_model = torch.quantization.prepare_qat(quat_model)\n",
    "print(type(quat_model))\n",
    "\n",
    "# print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',quat_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:08<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Acc : 90.06 acc\n",
      "Before Training\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.03 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:50<00:00,  7.70it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------0----------\n",
      "Train 0.6539 Loss, 94.07 Acc\n",
      "Validation 0.7404 Loss, 89.93 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------1----------\n",
      "Train 0.6514 Loss, 94.11 Acc\n",
      "Validation 0.7387 Loss, 90.03 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.94it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2----------\n",
      "Train 0.6492 Loss, 94.16 Acc\n",
      "Validation 0.7379 Loss, 90.11 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.93it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------3----------\n",
      "Train 0.6496 Loss, 94.15 Acc\n",
      "Validation 0.7385 Loss, 90.11 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.90it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------4----------\n",
      "Train 0.6481 Loss, 94.33 Acc\n",
      "Validation 0.7382 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------5----------\n",
      "Train 0.6473 Loss, 94.32 Acc\n",
      "Validation 0.7375 Loss, 90.19 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.02it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------6----------\n",
      "Train 0.6467 Loss, 94.40 Acc\n",
      "Validation 0.7382 Loss, 90.13 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------7----------\n",
      "Train 0.6456 Loss, 94.42 Acc\n",
      "Validation 0.7370 Loss, 90.19 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------8----------\n",
      "Train 0.6463 Loss, 94.31 Acc\n",
      "Validation 0.7370 Loss, 90.14 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------9----------\n",
      "Train 0.6459 Loss, 94.42 Acc\n",
      "Validation 0.7378 Loss, 90.08 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  7.99it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------10----------\n",
      "Train 0.6445 Loss, 94.48 Acc\n",
      "Validation 0.7379 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------11----------\n",
      "Train 0.6450 Loss, 94.32 Acc\n",
      "Validation 0.7373 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.92it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------12----------\n",
      "Train 0.6437 Loss, 94.40 Acc\n",
      "Validation 0.7357 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------13----------\n",
      "Train 0.6420 Loss, 94.55 Acc\n",
      "Validation 0.7363 Loss, 90.28 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------14----------\n",
      "Train 0.6418 Loss, 94.53 Acc\n",
      "Validation 0.7369 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.85it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------15----------\n",
      "Train 0.6427 Loss, 94.39 Acc\n",
      "Validation 0.7365 Loss, 90.33 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.98it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------16----------\n",
      "Train 0.6423 Loss, 94.50 Acc\n",
      "Validation 0.7359 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.89it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------17----------\n",
      "Train 0.6424 Loss, 94.56 Acc\n",
      "Validation 0.7355 Loss, 90.34 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.04it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------18----------\n",
      "Train 0.6417 Loss, 94.48 Acc\n",
      "Validation 0.7352 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.00it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------19----------\n",
      "Train 0.6415 Loss, 94.55 Acc\n",
      "Validation 0.7367 Loss, 90.32 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------20----------\n",
      "Train 0.6410 Loss, 94.57 Acc\n",
      "Validation 0.7367 Loss, 90.27 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  7.99it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------21----------\n",
      "Train 0.6407 Loss, 94.46 Acc\n",
      "Validation 0.7366 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.93it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------22----------\n",
      "Train 0.6386 Loss, 94.70 Acc\n",
      "Validation 0.7354 Loss, 90.23 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.04it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------23----------\n",
      "Train 0.6408 Loss, 94.48 Acc\n",
      "Validation 0.7362 Loss, 90.27 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------24----------\n",
      "Train 0.6400 Loss, 94.66 Acc\n",
      "Validation 0.7362 Loss, 90.22 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.03it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------25----------\n",
      "Train 0.6406 Loss, 94.58 Acc\n",
      "Validation 0.7354 Loss, 90.26 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.00it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------26----------\n",
      "Train 0.6407 Loss, 94.47 Acc\n",
      "Validation 0.7352 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.98it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------27----------\n",
      "Train 0.6408 Loss, 94.65 Acc\n",
      "Validation 0.7350 Loss, 90.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------28----------\n",
      "Train 0.6400 Loss, 94.66 Acc\n",
      "Validation 0.7348 Loss, 90.28 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.91it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------29----------\n",
      "Train 0.6367 Loss, 94.84 Acc\n",
      "Validation 0.7349 Loss, 90.33 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qat_need = True\n",
    "if qat_need:\n",
    "    optimizer = torch.optim.SGD(quat_model.parameters(), lr=1e-5, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,90], gamma=0.5)\n",
    "\n",
    "    first_loss, first_acc = Evaluating(model=quat_model,test_loader=Test_loader,device=cpu_device,criterion=nn.CrossEntropyLoss())\n",
    "    print(f\"Before Acc : {first_acc:.2f} acc\")\n",
    "    quat_model = Training(quat_model,train_loader=Train_loader,test_loader=Test_loader,\n",
    "    device=gpu_device,optimizer=optimizer,scheduler=scheduler,epochs=30,model_name=\"QAT_mobilenetv2_cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 24.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8_model acc : 88.47 %\n"
     ]
    }
   ],
   "source": [
    "# QAT가 적용된 floating point 모델을 quantized int model로 변환\n",
    "# quat_model.load_state_dict(torch.load(\"./models/QAT_mobilenetv2_cifar10.pt\"))\n",
    "quat_model.to('cpu')\n",
    "int8_model = torch.ao.quantization.convert(quat_model)\n",
    "int8_model.eval()\n",
    "_,int8_acc = Evaluating(int8_model,Test_loader,cpu_device)\n",
    "print(f\"int8_model acc : {int8_acc:.2f} %\")\n",
    "torch.jit.save(torch.jit.script(int8_model),\"./models/QAT_mobilenetv2_cifar10_jit.pt\")\n",
    "int8_model = torch.jit.load(\"./models/QAT_mobilenetv2_cifar10_jit.pt\",map_location=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit int8_model acc : 88.47 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_,int8_acc = Evaluating(int8_model,Test_loader,cpu_device)\n",
    "print(f\"jit int8_model acc : {int8_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post int8_model acc :85.65 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_model = torch.jit.load(\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "_,int8_acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "print(f\"post int8_model acc :{int8_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         QuantStub-1            [-1, 3, 32, 32]               0\n",
      "            Conv2d-2           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-3           [-1, 32, 16, 16]              64\n",
      "              ReLU-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-9           [-1, 16, 16, 16]              32\n",
      "QuantizableInvertedResidual-10           [-1, 16, 16, 16]               0\n",
      "           Conv2d-11           [-1, 64, 16, 16]           1,024\n",
      "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
      "             ReLU-13           [-1, 64, 16, 16]               0\n",
      "           Conv2d-14             [-1, 64, 8, 8]             576\n",
      "      BatchNorm2d-15             [-1, 64, 8, 8]             128\n",
      "             ReLU-16             [-1, 64, 8, 8]               0\n",
      "           Conv2d-17             [-1, 32, 8, 8]           2,048\n",
      "      BatchNorm2d-18             [-1, 32, 8, 8]              64\n",
      "QuantizableInvertedResidual-19             [-1, 32, 8, 8]               0\n",
      "           Conv2d-20            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-21            [-1, 128, 8, 8]             256\n",
      "             ReLU-22            [-1, 128, 8, 8]               0\n",
      "           Conv2d-23            [-1, 128, 8, 8]           1,152\n",
      "      BatchNorm2d-24            [-1, 128, 8, 8]             256\n",
      "             ReLU-25            [-1, 128, 8, 8]               0\n",
      "           Conv2d-26             [-1, 32, 8, 8]           4,096\n",
      "      BatchNorm2d-27             [-1, 32, 8, 8]              64\n",
      "         Identity-28             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-29             [-1, 32, 8, 8]               0\n",
      "           Conv2d-30            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-31            [-1, 128, 8, 8]             256\n",
      "             ReLU-32            [-1, 128, 8, 8]               0\n",
      "           Conv2d-33            [-1, 128, 8, 8]           1,152\n",
      "      BatchNorm2d-34            [-1, 128, 8, 8]             256\n",
      "             ReLU-35            [-1, 128, 8, 8]               0\n",
      "           Conv2d-36             [-1, 32, 8, 8]           4,096\n",
      "      BatchNorm2d-37             [-1, 32, 8, 8]              64\n",
      "         Identity-38             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-39             [-1, 32, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "             ReLU-42            [-1, 128, 8, 8]               0\n",
      "           Conv2d-43            [-1, 128, 4, 4]           1,152\n",
      "      BatchNorm2d-44            [-1, 128, 4, 4]             256\n",
      "             ReLU-45            [-1, 128, 4, 4]               0\n",
      "           Conv2d-46             [-1, 64, 4, 4]           8,192\n",
      "      BatchNorm2d-47             [-1, 64, 4, 4]             128\n",
      "QuantizableInvertedResidual-48             [-1, 64, 4, 4]               0\n",
      "           Conv2d-49            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-50            [-1, 256, 4, 4]             512\n",
      "             ReLU-51            [-1, 256, 4, 4]               0\n",
      "           Conv2d-52            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-53            [-1, 256, 4, 4]             512\n",
      "             ReLU-54            [-1, 256, 4, 4]               0\n",
      "           Conv2d-55             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-56             [-1, 64, 4, 4]             128\n",
      "         Identity-57             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-58             [-1, 64, 4, 4]               0\n",
      "           Conv2d-59            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-60            [-1, 256, 4, 4]             512\n",
      "             ReLU-61            [-1, 256, 4, 4]               0\n",
      "           Conv2d-62            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-63            [-1, 256, 4, 4]             512\n",
      "             ReLU-64            [-1, 256, 4, 4]               0\n",
      "           Conv2d-65             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
      "         Identity-67             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-68             [-1, 64, 4, 4]               0\n",
      "           Conv2d-69            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-70            [-1, 256, 4, 4]             512\n",
      "             ReLU-71            [-1, 256, 4, 4]               0\n",
      "           Conv2d-72            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-73            [-1, 256, 4, 4]             512\n",
      "             ReLU-74            [-1, 256, 4, 4]               0\n",
      "           Conv2d-75             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-76             [-1, 64, 4, 4]             128\n",
      "         Identity-77             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-78             [-1, 64, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "             ReLU-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]           2,304\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "             ReLU-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85            [-1, 128, 2, 2]          32,768\n",
      "      BatchNorm2d-86            [-1, 128, 2, 2]             256\n",
      "QuantizableInvertedResidual-87            [-1, 128, 2, 2]               0\n",
      "           Conv2d-88            [-1, 512, 2, 2]          65,536\n",
      "      BatchNorm2d-89            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-90            [-1, 512, 2, 2]               0\n",
      "           Conv2d-91            [-1, 512, 2, 2]           4,608\n",
      "      BatchNorm2d-92            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-93            [-1, 512, 2, 2]               0\n",
      "           Conv2d-94            [-1, 128, 2, 2]          65,536\n",
      "      BatchNorm2d-95            [-1, 128, 2, 2]             256\n",
      "         Identity-96            [-1, 128, 2, 2]               0\n",
      "QuantizableInvertedResidual-97            [-1, 128, 2, 2]               0\n",
      "           Conv2d-98            [-1, 512, 2, 2]          65,536\n",
      "      BatchNorm2d-99            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-100            [-1, 512, 2, 2]               0\n",
      "          Conv2d-101            [-1, 512, 2, 2]           4,608\n",
      "     BatchNorm2d-102            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-103            [-1, 512, 2, 2]               0\n",
      "          Conv2d-104            [-1, 128, 2, 2]          65,536\n",
      "     BatchNorm2d-105            [-1, 128, 2, 2]             256\n",
      "        Identity-106            [-1, 128, 2, 2]               0\n",
      "QuantizableInvertedResidual-107            [-1, 128, 2, 2]               0\n",
      "          Conv2d-108           [-1, 1280, 2, 2]         163,840\n",
      "     BatchNorm2d-109           [-1, 1280, 2, 2]           2,560\n",
      "            ReLU-110           [-1, 1280, 2, 2]               0\n",
      "         Dropout-111                 [-1, 1280]               0\n",
      "          Linear-112                   [-1, 10]          12,810\n",
      "     DeQuantStub-113                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 656,298\n",
      "Trainable params: 656,298\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.27\n",
      "Params size (MB): 2.50\n",
      "Estimated Total Size (MB): 5.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models import quat_mobilenet_v2\n",
    "from torchsummary import summary\n",
    "inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                # [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "                [4, 32, 3, 2],\n",
    "                [4, 64, 4, 2],\n",
    "                # [6, 96, 3, 1],\n",
    "                [4, 128, 3, 2],\n",
    "                # [6, 320, 1, 1],\n",
    "            ]\n",
    "tiny_mobilenet = quat_mobilenet_v2(cifar10=True)\n",
    "summary(tiny_mobilenet,(3,32,32),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.0 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.83it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------1----------\n",
      "Train 1.9100 Loss, 32.71 Acc\n",
      "Validation 1.7708 Loss, 40.71 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.78it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2----------\n",
      "Train 1.6648 Loss, 45.58 Acc\n",
      "Validation 1.5759 Loss, 50.83 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.79it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------3----------\n",
      "Train 1.5527 Loss, 51.65 Acc\n",
      "Validation 1.4685 Loss, 55.61 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.83it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------4----------\n",
      "Train 1.4661 Loss, 55.93 Acc\n",
      "Validation 1.4088 Loss, 58.74 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------5----------\n",
      "Train 1.3946 Loss, 59.35 Acc\n",
      "Validation 1.3212 Loss, 62.56 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------6----------\n",
      "Train 1.3385 Loss, 62.42 Acc\n",
      "Validation 1.2739 Loss, 64.98 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.81it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------7----------\n",
      "Train 1.2960 Loss, 64.35 Acc\n",
      "Validation 1.2375 Loss, 66.87 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------8----------\n",
      "Train 1.2587 Loss, 65.83 Acc\n",
      "Validation 1.2230 Loss, 68.05 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.79it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------9----------\n",
      "Train 1.2233 Loss, 67.60 Acc\n",
      "Validation 1.1985 Loss, 68.92 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.79it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------10----------\n",
      "Train 1.1964 Loss, 68.64 Acc\n",
      "Validation 1.1538 Loss, 71.13 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------11----------\n",
      "Train 1.1320 Loss, 71.90 Acc\n",
      "Validation 1.0916 Loss, 73.40 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 11.95it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------12----------\n",
      "Train 1.1131 Loss, 72.54 Acc\n",
      "Validation 1.0825 Loss, 73.94 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------13----------\n",
      "Train 1.0969 Loss, 73.38 Acc\n",
      "Validation 1.0678 Loss, 74.48 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.88it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------14----------\n",
      "Train 1.0834 Loss, 74.13 Acc\n",
      "Validation 1.0669 Loss, 75.01 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------15----------\n",
      "Train 1.0716 Loss, 74.58 Acc\n",
      "Validation 1.0620 Loss, 74.85 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------16----------\n",
      "Train 1.0623 Loss, 74.87 Acc\n",
      "Validation 1.0511 Loss, 75.26 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.75it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------17----------\n",
      "Train 1.0474 Loss, 75.70 Acc\n",
      "Validation 1.0421 Loss, 75.89 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------18----------\n",
      "Train 1.0407 Loss, 76.17 Acc\n",
      "Validation 1.0388 Loss, 76.11 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.83it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------19----------\n",
      "Train 1.0307 Loss, 76.49 Acc\n",
      "Validation 1.0274 Loss, 76.48 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.76it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------20----------\n",
      "Train 1.0187 Loss, 76.98 Acc\n",
      "Validation 1.0115 Loss, 77.58 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.54it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------21----------\n",
      "Train 1.0130 Loss, 77.27 Acc\n",
      "Validation 1.0090 Loss, 77.57 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.83it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------22----------\n",
      "Train 1.0026 Loss, 77.80 Acc\n",
      "Validation 1.0092 Loss, 77.56 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.57it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------23----------\n",
      "Train 0.9945 Loss, 78.22 Acc\n",
      "Validation 0.9930 Loss, 78.23 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------24----------\n",
      "Train 0.9853 Loss, 78.58 Acc\n",
      "Validation 1.0002 Loss, 77.97 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.85it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------25----------\n",
      "Train 0.9791 Loss, 78.91 Acc\n",
      "Validation 0.9812 Loss, 78.68 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.45it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------26----------\n",
      "Train 0.9742 Loss, 79.15 Acc\n",
      "Validation 0.9862 Loss, 78.55 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.81it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------27----------\n",
      "Train 0.9624 Loss, 79.70 Acc\n",
      "Validation 0.9706 Loss, 78.91 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------28----------\n",
      "Train 0.9600 Loss, 79.85 Acc\n",
      "Validation 0.9865 Loss, 78.21 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------29----------\n",
      "Train 0.9512 Loss, 80.26 Acc\n",
      "Validation 0.9725 Loss, 79.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.90it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------30----------\n",
      "Train 0.9451 Loss, 80.40 Acc\n",
      "Validation 0.9684 Loss, 79.34 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.79it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------31----------\n",
      "Train 0.9203 Loss, 81.63 Acc\n",
      "Validation 0.9420 Loss, 80.88 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------32----------\n",
      "Train 0.9110 Loss, 81.85 Acc\n",
      "Validation 0.9369 Loss, 81.04 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.96it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------33----------\n",
      "Train 0.9058 Loss, 82.13 Acc\n",
      "Validation 0.9488 Loss, 80.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.99it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------34----------\n",
      "Train 0.8996 Loss, 82.72 Acc\n",
      "Validation 0.9462 Loss, 80.93 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------35----------\n",
      "Train 0.8938 Loss, 82.77 Acc\n",
      "Validation 0.9369 Loss, 81.12 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------36----------\n",
      "Train 0.8989 Loss, 82.67 Acc\n",
      "Validation 0.9305 Loss, 81.13 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------37----------\n",
      "Train 0.8906 Loss, 82.84 Acc\n",
      "Validation 0.9267 Loss, 81.46 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.69it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------38----------\n",
      "Train 0.8850 Loss, 83.00 Acc\n",
      "Validation 0.9364 Loss, 80.79 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.78it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------39----------\n",
      "Train 0.8843 Loss, 83.13 Acc\n",
      "Validation 0.9355 Loss, 80.94 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.73it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------40----------\n",
      "Train 0.8821 Loss, 83.22 Acc\n",
      "Validation 0.9298 Loss, 81.45 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.51it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------41----------\n",
      "Train 0.8759 Loss, 83.56 Acc\n",
      "Validation 0.9276 Loss, 81.22 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.83it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------42----------\n",
      "Train 0.8736 Loss, 83.80 Acc\n",
      "Validation 0.9255 Loss, 81.40 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.97it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------43----------\n",
      "Train 0.8714 Loss, 83.91 Acc\n",
      "Validation 0.9321 Loss, 81.52 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.87it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------44----------\n",
      "Train 0.8654 Loss, 84.09 Acc\n",
      "Validation 0.9287 Loss, 81.26 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------45----------\n",
      "Train 0.8658 Loss, 84.08 Acc\n",
      "Validation 0.9334 Loss, 81.37 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------46----------\n",
      "Train 0.8634 Loss, 84.16 Acc\n",
      "Validation 0.9299 Loss, 81.06 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.88it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------47----------\n",
      "Train 0.8592 Loss, 84.42 Acc\n",
      "Validation 0.9262 Loss, 81.57 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.85it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------48----------\n",
      "Train 0.8582 Loss, 84.47 Acc\n",
      "Validation 0.9212 Loss, 81.47 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.78it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------49----------\n",
      "Train 0.8588 Loss, 84.38 Acc\n",
      "Validation 0.9221 Loss, 81.53 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------50----------\n",
      "Train 0.8514 Loss, 84.75 Acc\n",
      "Validation 0.9298 Loss, 81.07 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------51----------\n",
      "Train 0.8492 Loss, 84.68 Acc\n",
      "Validation 0.9186 Loss, 81.71 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------52----------\n",
      "Train 0.8455 Loss, 84.83 Acc\n",
      "Validation 0.9167 Loss, 81.53 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.89it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------53----------\n",
      "Train 0.8447 Loss, 84.78 Acc\n",
      "Validation 0.9129 Loss, 81.97 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.89it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------54----------\n",
      "Train 0.8382 Loss, 85.21 Acc\n",
      "Validation 0.9150 Loss, 82.17 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.86it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------55----------\n",
      "Train 0.8399 Loss, 85.10 Acc\n",
      "Validation 0.9189 Loss, 82.02 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:32<00:00, 11.96it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------56----------\n",
      "Train 0.8355 Loss, 85.34 Acc\n",
      "Validation 0.9135 Loss, 82.05 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.89it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------57----------\n",
      "Train 0.8322 Loss, 85.79 Acc\n",
      "Validation 0.9047 Loss, 82.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.86it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------58----------\n",
      "Train 0.8298 Loss, 85.65 Acc\n",
      "Validation 0.9076 Loss, 82.37 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.97it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------59----------\n",
      "Train 0.8296 Loss, 85.61 Acc\n",
      "Validation 0.9137 Loss, 81.90 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------60----------\n",
      "Train 0.8249 Loss, 85.90 Acc\n",
      "Validation 0.9115 Loss, 81.91 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------61----------\n",
      "Train 0.8273 Loss, 85.78 Acc\n",
      "Validation 0.9074 Loss, 82.44 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------62----------\n",
      "Train 0.8212 Loss, 86.00 Acc\n",
      "Validation 0.9069 Loss, 82.48 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------63----------\n",
      "Train 0.8216 Loss, 86.20 Acc\n",
      "Validation 0.9068 Loss, 82.35 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.75it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------64----------\n",
      "Train 0.8186 Loss, 86.11 Acc\n",
      "Validation 0.9021 Loss, 82.64 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.70it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------65----------\n",
      "Train 0.8149 Loss, 86.31 Acc\n",
      "Validation 0.9134 Loss, 82.04 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.77it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------66----------\n",
      "Train 0.8131 Loss, 86.50 Acc\n",
      "Validation 0.8978 Loss, 82.95 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.71it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------67----------\n",
      "Train 0.8127 Loss, 86.38 Acc\n",
      "Validation 0.9064 Loss, 82.53 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.76it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------68----------\n",
      "Train 0.8110 Loss, 86.46 Acc\n",
      "Validation 0.8988 Loss, 82.99 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.76it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------69----------\n",
      "Train 0.8080 Loss, 86.57 Acc\n",
      "Validation 0.9209 Loss, 81.76 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------70----------\n",
      "Train 0.8072 Loss, 86.68 Acc\n",
      "Validation 0.9056 Loss, 82.78 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.90it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------71----------\n",
      "Train 0.7860 Loss, 87.66 Acc\n",
      "Validation 0.8894 Loss, 83.36 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.74it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------72----------\n",
      "Train 0.7791 Loss, 87.84 Acc\n",
      "Validation 0.8869 Loss, 83.51 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.71it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------73----------\n",
      "Train 0.7766 Loss, 88.02 Acc\n",
      "Validation 0.8831 Loss, 83.84 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.79it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------74----------\n",
      "Train 0.7721 Loss, 88.33 Acc\n",
      "Validation 0.8827 Loss, 83.85 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.58it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------75----------\n",
      "Train 0.7707 Loss, 88.36 Acc\n",
      "Validation 0.8879 Loss, 83.67 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.54it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------76----------\n",
      "Train 0.7697 Loss, 88.41 Acc\n",
      "Validation 0.8829 Loss, 83.88 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.46it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------77----------\n",
      "Train 0.7720 Loss, 88.28 Acc\n",
      "Validation 0.8894 Loss, 83.33 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.51it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------78----------\n",
      "Train 0.7701 Loss, 88.52 Acc\n",
      "Validation 0.8903 Loss, 83.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.51it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------79----------\n",
      "Train 0.7675 Loss, 88.60 Acc\n",
      "Validation 0.8875 Loss, 83.66 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.71it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------80----------\n",
      "Train 0.7644 Loss, 88.65 Acc\n",
      "Validation 0.8864 Loss, 83.48 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------81----------\n",
      "Train 0.7640 Loss, 88.74 Acc\n",
      "Validation 0.8901 Loss, 83.52 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.87it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------82----------\n",
      "Train 0.7590 Loss, 88.86 Acc\n",
      "Validation 0.8891 Loss, 83.70 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------83----------\n",
      "Train 0.7601 Loss, 88.81 Acc\n",
      "Validation 0.8865 Loss, 83.68 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------84----------\n",
      "Train 0.7586 Loss, 89.02 Acc\n",
      "Validation 0.8820 Loss, 84.03 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.64it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------85----------\n",
      "Train 0.7593 Loss, 88.98 Acc\n",
      "Validation 0.8865 Loss, 83.55 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.81it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------86----------\n",
      "Train 0.7545 Loss, 89.24 Acc\n",
      "Validation 0.8870 Loss, 83.75 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.71it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------87----------\n",
      "Train 0.7573 Loss, 88.95 Acc\n",
      "Validation 0.8939 Loss, 83.35 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.84it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------88----------\n",
      "Train 0.7566 Loss, 88.98 Acc\n",
      "Validation 0.8855 Loss, 83.70 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------89----------\n",
      "Train 0.7535 Loss, 89.22 Acc\n",
      "Validation 0.8913 Loss, 83.41 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.90it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------90----------\n",
      "Train 0.7568 Loss, 88.90 Acc\n",
      "Validation 0.8858 Loss, 83.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:35<00:00, 10.86it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------91----------\n",
      "Train 0.7545 Loss, 89.11 Acc\n",
      "Validation 0.8904 Loss, 83.52 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:36<00:00, 10.78it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------92----------\n",
      "Train 0.7517 Loss, 89.20 Acc\n",
      "Validation 0.8868 Loss, 83.95 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.56it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 17.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------93----------\n",
      "Train 0.7490 Loss, 89.41 Acc\n",
      "Validation 0.8878 Loss, 83.74 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.53it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------94----------\n",
      "Train 0.7495 Loss, 89.36 Acc\n",
      "Validation 0.8862 Loss, 83.99 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:37<00:00, 10.47it/s]\n",
      "100%|██████████| 79/79 [00:04<00:00, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------95----------\n",
      "Train 0.7486 Loss, 89.37 Acc\n",
      "Validation 0.8894 Loss, 83.75 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(tiny_mobilenet.parameters(), lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,70], gamma=0.5)\n",
    "\n",
    "tiny_mobilenet = Training(tiny_mobilenet,Train_loader,Test_loader,gpu_device,optimizer,scheduler,model_name=\"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 24.98it/s]\n",
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization acc : 84.03 %\n",
      "Q config = QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "calibrating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1135: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "100%|██████████| 79/79 [00:02<00:00, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post int8_model acc :83.79 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def _replace_relu(module: nn.Module) -> None:\n",
    "    from models.layers import Quant_ReLU\n",
    "    reassign = {}\n",
    "    for name, mod in module.named_children():\n",
    "        _replace_relu(mod)\n",
    "        # Checking for explicit type instead of instance\n",
    "        # as we only want to replace modules of the exact type\n",
    "        # not inherited classes\n",
    "        if type(mod) is nn.ReLU or type(mod) is nn.ReLU6 or type(mod) is Quant_ReLU:\n",
    "            reassign[name] = nn.ReLU(inplace=False)\n",
    "\n",
    "    for key, value in reassign.items():\n",
    "        module._modules[key] = value\n",
    "def fused_model(model,is_qat = None) -> None:\n",
    "    from models import Conv2dNormActivation, QuantizableInvertedResidual\n",
    "    for m in model.modules():\n",
    "        if type(m) is Conv2dNormActivation:\n",
    "            torch.ao.quantization.fuse_modules(m, [\"0\", \"1\", \"2\"],inplace=True)\n",
    "        if type(m) is QuantizableInvertedResidual:\n",
    "            m.fuse_model(is_qat)\n",
    "            \n",
    "tiny_mobilenet = quat_mobilenet_v2(cifar10=True)\n",
    "tiny_mobilenet = tiny_mobilenet.to(\"cpu\")\n",
    "tiny_mobilenet.load_state_dict(torch.load(\"./models/tiny_mobilenetv2_cifar.pt\"))\n",
    "# from utils import custom_quant_weights\n",
    "# tiny_mobilenet, _ = custom_quant_weights(tiny_mobilenet)\n",
    "\n",
    "_,acc = Evaluating(tiny_mobilenet,Test_loader,\"cpu\")\n",
    "print(f\"Before quantization acc : {acc:.2f} %\")\n",
    "\n",
    "tiny_quant = copy.deepcopy(tiny_mobilenet)\n",
    "\n",
    "from models import quantize_model\n",
    "qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantize_model(tiny_quant, data= Train_loader,qconfig=qconfig)\n",
    "\n",
    "# torch.jit.save(torch.jit.script(tiny_quant),\"./models/Q_tiny.pt\")\n",
    "# tiny_quant = torch.jit.load(\"./models/Q_tiny.pt\")\n",
    "_,int8_acc = Evaluating(tiny_quant,Test_loader,\"cpu\")\n",
    "print(f\"post int8_model acc :{int8_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c3f2c56578ca93d34255952c36601b421b6ecc37a0d9982e6b92a246c07692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
