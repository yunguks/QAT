{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 1.12.1\n",
      "torchvision = 0.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def memory_check():\n",
    "    print(f\"  Allocated: {round(torch.cuda.memory_allocated()/1024**3,2)} GB\")\n",
    "    print(f\"  Cached:    {round(torch.cuda.memory_reserved()/1024**3,2)} GB\\n\")\n",
    "\n",
    "print(f\"torch = {torch.__version__}\")\n",
    "print(f\"torchvision = {torchvision.__version__}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ImageNet(validation 6G) Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "def ImageNet_DataLoader(split_num = [0.08,0.02,0.9]):\n",
    "    if not os.path.exists(\"./data/ImageNet/meta.bin\"):\n",
    "        print(\"Meta data download\")\n",
    "        wget.download(url=\"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\", out=\"./data/ImageNet\")\n",
    "    # if not os.path.exists(\"./data/ImageNet/ILSVRC2012_devkit_t3.tar.gz\"):\n",
    "    #     print(\"Toolkit t3 Download\")\n",
    "    #     toolkit_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t3.tar.gz\"\n",
    "    #     wget.download(url= toolkit_url,out=\"./data/ImageNet\")\n",
    "    if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_val.tar\"):\n",
    "        print(\"Download val data\")\n",
    "        val_url  = 'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar'\n",
    "        wget.download(url=val_url, out=\"./data/ImageNet\")\n",
    "\n",
    "    # if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_train_t3.tar\"):\n",
    "    #     print(\"Download train t3 data\")\n",
    "    #     train_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train_t3.tar\"\n",
    "    #     wget.download(url=train_url,out=\"./data/ImageNet\")\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    print(os.getcwd())\n",
    "    dataset = torchvision.datasets.ImageNet(root=\"./data/ImageNet\",split=\"val\", transform = train_transform)\n",
    "    Train_dataset, Test_dataset,_ = torch.utils.data.random_split(dataset, split_num)\n",
    "    print(f\"Train data set = {len(Train_dataset)}, Test = {len(Test_dataset)}\")\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(Train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(Test_dataset)\n",
    "\n",
    "    Train_loader = torch.utils.data.DataLoader(dataset=Train_dataset, batch_size= 32, sampler = train_sampler)\n",
    "    Test_loader = torch.utils.data.DataLoader(dataset=Test_dataset, batch_size =32, sampler = test_sampler)\n",
    "    return Train_loader, Test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cifar10_Dataloader():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding = 4),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "    print(f\"Train data set = {len(train_dataset)}, Test = {len(test_dataset)}\")\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=128,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_dataset, batch_size=128,\n",
    "        sampler=test_sampler)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Fuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluating(model, test_loader, device, criterion=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(iter(test_loader)):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "        # statistics\n",
    "        running_loss += loss * labels.size(0)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = 100 * running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(model, train_loader, test_loader, device, optimizer, scheduler, epochs=100,model_name=\"test\"):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    print(\"Before Training\")\n",
    "    torch.cuda.memory_reserved()\n",
    "    memory_check()\n",
    "    count = 0\n",
    "    best_loss = np.Inf\n",
    "    # Training\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in tqdm(iter(train_loader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    " \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # statistics\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            del preds\n",
    "        # Set learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * running_corrects / len(train_loader.dataset) \n",
    "\n",
    "        # Evaluation\n",
    "        val_loss, val_acc = Evaluating(model,test_loader,device=device,criterion=criterion)\n",
    "        print(f\"--------{epoch}----------\")\n",
    "        print(f\"Train {train_loss:.4f} Loss, {train_accuracy:.2f} Acc\")\n",
    "        print(f\"Validation {val_loss:.4f} Loss, {val_acc:.2f} Acc\")\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            count = 0\n",
    "            torch.save(model.state_dict(), f\"./models/{model_name}.pt\")\n",
    "        else:\n",
    "            count +=1\n",
    "            if count > 10:\n",
    "                break\n",
    "    model.load_state_dict(torch.load(f\"./models/{model_name}.pt\")) \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer fusion Check\n",
    "conv, bn, relu를 하나의 layer로 만들어 각각의 layer를 읽어오는 연산을 줄이는 과정   \n",
    "folding과는 다른 경량화 기법   \n",
    "Fusion 된 layer는 identity로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eq_check(model1, model2, device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model1(x).detach().cpu().numpy()\n",
    "        y2 = model2(x).detach().cpu().numpy()\n",
    "        # 배열이 허용 오차범위 abs(a - b) <= (atol + rtol * absolute(b)) 이내면 True\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test fail\")\n",
    "            return False\n",
    "    print(\"Two models equal\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(model, device, input_size = (1,3,256,256),num_tests=100,):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(num_tests):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "    aver_time = total_time / num_tests\n",
    "    return total_time, aver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnReLUModel(\n",
      "  (conv): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "ConvBnReLUModel(\n",
      "  (conv): ConvReLU2d(\n",
      "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn): Identity()\n",
      "  (relu): Identity()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "-- Equal Test --\n",
      "Two models equal\n",
      "-- Infer Time Test --\n",
      "origin model infer time 0.054s\n",
      "fusion model infer time 0.042s\n"
     ]
    }
   ],
   "source": [
    "class ConvBnReLUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBnReLUModel,self).__init__()\n",
    "        self.conv = nn.Conv2d(3,5,3,bias=True).to(dtype=torch.float)\n",
    "        self.bn = nn.BatchNorm2d(5).to(dtype=torch.float)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "model = ConvBnReLUModel().to(device=torch.device(\"cpu:0\"))\n",
    "model.eval()\n",
    "print(model)\n",
    "# for p in model.named_parameters():\n",
    "#     print(p)\n",
    "#     print()\n",
    "# \"fbgemm\" for server , \"qnnpack\" for mobile \n",
    "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# # torch.quantization.fuse_module or myModel.fuse_model()\n",
    "fuse_model = torch.ao.quantization.fuse_modules(model,[['conv','bn','relu']], inplace=False)\n",
    "# fuse_model = model.fuse_model()\n",
    "print(fuse_model)\n",
    "\n",
    "print(f\"-- Equal Test --\")\n",
    "model_eq_check(model, fuse_model, device=torch.device(\"cpu:0\"))\n",
    "\n",
    "\n",
    "print(f\"-- Infer Time Test --\")\n",
    "ori_cpu_time,_ = time_test(model,torch.device(\"cpu\"))\n",
    "fus_cpu_time,_ = time_test(fuse_model,torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"origin model infer time {ori_cpu_time:.3f}s\")\n",
    "print(f\"fusion model infer time {fus_cpu_time:.3f}s\")\n",
    "del model\n",
    "del fuse_model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         QuantStub-1            [-1, 3, 32, 32]               0\n",
      "            Conv2d-2           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-3           [-1, 32, 16, 16]              64\n",
      "              ReLU-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "              ReLU-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-9           [-1, 16, 16, 16]              32\n",
      "QuantizableInvertedResidual-10           [-1, 16, 16, 16]               0\n",
      "           Conv2d-11           [-1, 96, 16, 16]           1,536\n",
      "      BatchNorm2d-12           [-1, 96, 16, 16]             192\n",
      "             ReLU-13           [-1, 96, 16, 16]               0\n",
      "           Conv2d-14           [-1, 96, 16, 16]             864\n",
      "      BatchNorm2d-15           [-1, 96, 16, 16]             192\n",
      "             ReLU-16           [-1, 96, 16, 16]               0\n",
      "           Conv2d-17           [-1, 24, 16, 16]           2,304\n",
      "      BatchNorm2d-18           [-1, 24, 16, 16]              48\n",
      "QuantizableInvertedResidual-19           [-1, 24, 16, 16]               0\n",
      "           Conv2d-20          [-1, 144, 16, 16]           3,456\n",
      "      BatchNorm2d-21          [-1, 144, 16, 16]             288\n",
      "             ReLU-22          [-1, 144, 16, 16]               0\n",
      "           Conv2d-23          [-1, 144, 16, 16]           1,296\n",
      "      BatchNorm2d-24          [-1, 144, 16, 16]             288\n",
      "             ReLU-25          [-1, 144, 16, 16]               0\n",
      "           Conv2d-26           [-1, 24, 16, 16]           3,456\n",
      "      BatchNorm2d-27           [-1, 24, 16, 16]              48\n",
      "         Identity-28           [-1, 24, 16, 16]               0\n",
      "QuantizableInvertedResidual-29           [-1, 24, 16, 16]               0\n",
      "           Conv2d-30          [-1, 144, 16, 16]           3,456\n",
      "      BatchNorm2d-31          [-1, 144, 16, 16]             288\n",
      "             ReLU-32          [-1, 144, 16, 16]               0\n",
      "           Conv2d-33            [-1, 144, 8, 8]           1,296\n",
      "      BatchNorm2d-34            [-1, 144, 8, 8]             288\n",
      "             ReLU-35            [-1, 144, 8, 8]               0\n",
      "           Conv2d-36             [-1, 32, 8, 8]           4,608\n",
      "      BatchNorm2d-37             [-1, 32, 8, 8]              64\n",
      "QuantizableInvertedResidual-38             [-1, 32, 8, 8]               0\n",
      "           Conv2d-39            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-40            [-1, 192, 8, 8]             384\n",
      "             ReLU-41            [-1, 192, 8, 8]               0\n",
      "           Conv2d-42            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-43            [-1, 192, 8, 8]             384\n",
      "             ReLU-44            [-1, 192, 8, 8]               0\n",
      "           Conv2d-45             [-1, 32, 8, 8]           6,144\n",
      "      BatchNorm2d-46             [-1, 32, 8, 8]              64\n",
      "         Identity-47             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-48             [-1, 32, 8, 8]               0\n",
      "           Conv2d-49            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-50            [-1, 192, 8, 8]             384\n",
      "             ReLU-51            [-1, 192, 8, 8]               0\n",
      "           Conv2d-52            [-1, 192, 8, 8]           1,728\n",
      "      BatchNorm2d-53            [-1, 192, 8, 8]             384\n",
      "             ReLU-54            [-1, 192, 8, 8]               0\n",
      "           Conv2d-55             [-1, 32, 8, 8]           6,144\n",
      "      BatchNorm2d-56             [-1, 32, 8, 8]              64\n",
      "         Identity-57             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-58             [-1, 32, 8, 8]               0\n",
      "           Conv2d-59            [-1, 192, 8, 8]           6,144\n",
      "      BatchNorm2d-60            [-1, 192, 8, 8]             384\n",
      "             ReLU-61            [-1, 192, 8, 8]               0\n",
      "           Conv2d-62            [-1, 192, 4, 4]           1,728\n",
      "      BatchNorm2d-63            [-1, 192, 4, 4]             384\n",
      "             ReLU-64            [-1, 192, 4, 4]               0\n",
      "           Conv2d-65             [-1, 64, 4, 4]          12,288\n",
      "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
      "QuantizableInvertedResidual-67             [-1, 64, 4, 4]               0\n",
      "           Conv2d-68            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-69            [-1, 384, 4, 4]             768\n",
      "             ReLU-70            [-1, 384, 4, 4]               0\n",
      "           Conv2d-71            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-72            [-1, 384, 4, 4]             768\n",
      "             ReLU-73            [-1, 384, 4, 4]               0\n",
      "           Conv2d-74             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-75             [-1, 64, 4, 4]             128\n",
      "         Identity-76             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-77             [-1, 64, 4, 4]               0\n",
      "           Conv2d-78            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-79            [-1, 384, 4, 4]             768\n",
      "             ReLU-80            [-1, 384, 4, 4]               0\n",
      "           Conv2d-81            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-82            [-1, 384, 4, 4]             768\n",
      "             ReLU-83            [-1, 384, 4, 4]               0\n",
      "           Conv2d-84             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-85             [-1, 64, 4, 4]             128\n",
      "         Identity-86             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-87             [-1, 64, 4, 4]               0\n",
      "           Conv2d-88            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-89            [-1, 384, 4, 4]             768\n",
      "             ReLU-90            [-1, 384, 4, 4]               0\n",
      "           Conv2d-91            [-1, 384, 4, 4]           3,456\n",
      "      BatchNorm2d-92            [-1, 384, 4, 4]             768\n",
      "             ReLU-93            [-1, 384, 4, 4]               0\n",
      "           Conv2d-94             [-1, 64, 4, 4]          24,576\n",
      "      BatchNorm2d-95             [-1, 64, 4, 4]             128\n",
      "         Identity-96             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-97             [-1, 64, 4, 4]               0\n",
      "           Conv2d-98            [-1, 384, 4, 4]          24,576\n",
      "      BatchNorm2d-99            [-1, 384, 4, 4]             768\n",
      "            ReLU-100            [-1, 384, 4, 4]               0\n",
      "          Conv2d-101            [-1, 384, 4, 4]           3,456\n",
      "     BatchNorm2d-102            [-1, 384, 4, 4]             768\n",
      "            ReLU-103            [-1, 384, 4, 4]               0\n",
      "          Conv2d-104             [-1, 96, 4, 4]          36,864\n",
      "     BatchNorm2d-105             [-1, 96, 4, 4]             192\n",
      "QuantizableInvertedResidual-106             [-1, 96, 4, 4]               0\n",
      "          Conv2d-107            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-108            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-109            [-1, 576, 4, 4]               0\n",
      "          Conv2d-110            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-111            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-112            [-1, 576, 4, 4]               0\n",
      "          Conv2d-113             [-1, 96, 4, 4]          55,296\n",
      "     BatchNorm2d-114             [-1, 96, 4, 4]             192\n",
      "        Identity-115             [-1, 96, 4, 4]               0\n",
      "QuantizableInvertedResidual-116             [-1, 96, 4, 4]               0\n",
      "          Conv2d-117            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-118            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-119            [-1, 576, 4, 4]               0\n",
      "          Conv2d-120            [-1, 576, 4, 4]           5,184\n",
      "     BatchNorm2d-121            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-122            [-1, 576, 4, 4]               0\n",
      "          Conv2d-123             [-1, 96, 4, 4]          55,296\n",
      "     BatchNorm2d-124             [-1, 96, 4, 4]             192\n",
      "        Identity-125             [-1, 96, 4, 4]               0\n",
      "QuantizableInvertedResidual-126             [-1, 96, 4, 4]               0\n",
      "          Conv2d-127            [-1, 576, 4, 4]          55,296\n",
      "     BatchNorm2d-128            [-1, 576, 4, 4]           1,152\n",
      "            ReLU-129            [-1, 576, 4, 4]               0\n",
      "          Conv2d-130            [-1, 576, 2, 2]           5,184\n",
      "     BatchNorm2d-131            [-1, 576, 2, 2]           1,152\n",
      "            ReLU-132            [-1, 576, 2, 2]               0\n",
      "          Conv2d-133            [-1, 160, 2, 2]          92,160\n",
      "     BatchNorm2d-134            [-1, 160, 2, 2]             320\n",
      "QuantizableInvertedResidual-135            [-1, 160, 2, 2]               0\n",
      "          Conv2d-136            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-138            [-1, 960, 2, 2]               0\n",
      "          Conv2d-139            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-141            [-1, 960, 2, 2]               0\n",
      "          Conv2d-142            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 2, 2]             320\n",
      "        Identity-144            [-1, 160, 2, 2]               0\n",
      "QuantizableInvertedResidual-145            [-1, 160, 2, 2]               0\n",
      "          Conv2d-146            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-147            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-148            [-1, 960, 2, 2]               0\n",
      "          Conv2d-149            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-150            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-151            [-1, 960, 2, 2]               0\n",
      "          Conv2d-152            [-1, 160, 2, 2]         153,600\n",
      "     BatchNorm2d-153            [-1, 160, 2, 2]             320\n",
      "        Identity-154            [-1, 160, 2, 2]               0\n",
      "QuantizableInvertedResidual-155            [-1, 160, 2, 2]               0\n",
      "          Conv2d-156            [-1, 960, 2, 2]         153,600\n",
      "     BatchNorm2d-157            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-158            [-1, 960, 2, 2]               0\n",
      "          Conv2d-159            [-1, 960, 2, 2]           8,640\n",
      "     BatchNorm2d-160            [-1, 960, 2, 2]           1,920\n",
      "            ReLU-161            [-1, 960, 2, 2]               0\n",
      "          Conv2d-162            [-1, 320, 2, 2]         307,200\n",
      "     BatchNorm2d-163            [-1, 320, 2, 2]             640\n",
      "QuantizableInvertedResidual-164            [-1, 320, 2, 2]               0\n",
      "          Conv2d-165           [-1, 1280, 2, 2]         409,600\n",
      "     BatchNorm2d-166           [-1, 1280, 2, 2]           2,560\n",
      "            ReLU-167           [-1, 1280, 2, 2]               0\n",
      "         Dropout-168                 [-1, 1280]               0\n",
      "          Linear-169                 [-1, 1000]       1,281,000\n",
      "         Dropout-170                 [-1, 1000]               0\n",
      "          Linear-171                   [-1, 10]          10,010\n",
      "     DeQuantStub-172                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 3,514,882\n",
      "Trainable params: 3,514,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.57\n",
      "Params size (MB): 13.41\n",
      "Estimated Total Size (MB): 22.99\n",
      "----------------------------------------------------------------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train data set = 50000, Test = 10000\n"
     ]
    }
   ],
   "source": [
    "# gpu,cpu device 선언\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# model 가져오기\n",
    "from models import mobilenet_v2, MobileNet_V2_Weights,quat_mobilenet_v2\n",
    "model = quat_mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1,activation_layer=nn.ReLU)\n",
    "model.classifier.append(nn.Dropout(0.2))\n",
    "model.classifier.append(nn.Linear(1000, 10))\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model,(3,32,32), device='cpu') \n",
    "\n",
    "# Move the model to CPU since static quantization does not support CUDA currently.\n",
    "# ImageNet Data \n",
    "Train_loader, Test_loader = Cifar10_Dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "1 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "2 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "3 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "4 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "5 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "6 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "7 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "8 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "9 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "10 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "11 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "12 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "13 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "14 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "15 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "16 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "17 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "18 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "19 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "20 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "21 data size = torch.Size([128, 3, 32, 32]), label size = torch.Size([128])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(Train_loader):\n",
    "\n",
    "    img = data[0].to(gpu_device)\n",
    "    label = data[1].to(gpu_device)\n",
    "    print(f\"{i} data size = {img.size()}, label size = {label.size()}\")\n",
    "    memory_check()\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:05<00:00, 15.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model acc : 90.06 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizableMobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (7): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (8): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (9): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (10): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (11): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (12): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (13): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (14): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (15): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (16): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (17): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  )\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_train=False\n",
    "if need_train:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,90], gamma=0.5)\n",
    "\n",
    "    model = Training(model,train_loader=Train_loader,test_loader=Test_loader,device=gpu_device,optimizer=optimizer,scheduler=scheduler,epochs=20,\n",
    "    model_name = \"q_mobilenetv2_cifar10\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"./models/q_mobilenetv2_cifar10.pt\"))\n",
    "    _,pre_acc = Evaluating(model,Test_loader,cpu_device)\n",
    "    print(f\"pretrained model acc : {pre_acc:.2f} %\")\n",
    "    # QAT가 적용된 floating point 모델을 quantized int model로 변환\n",
    "model.to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizableMobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (11): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (14): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (17): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Equal Test between origin and fused\n",
      "Two models equal\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델을 CPU상태로 두고 eval로 layer fusion\n",
    "model.eval()\n",
    "print(model)\n",
    "# Layer fusion\n",
    "# fused_model = torch.quantization.fuse_modules(model,[[\"conv1\",\"bn1\",\"relu\"]])\n",
    "\n",
    "# for module_name, module in fused_model.named_children():\n",
    "#     if \"layer\" in module_name:\n",
    "#         # basic_block 의 conv1, bn1, relu, conv2, bn2 를 fusion\n",
    "#         for basic_block_name, basic_block in module.named_children():\n",
    "#             torch.ao.quantization.fuse_modules(basic_block,[[\"conv1\",\"bn1\",\"relu\"],[\"conv2\",\"bn2\"]],inplace=True)\n",
    "#             # basic_block안의 downsampling block의 Conv2d Batchnorm2D fusion\n",
    "#             for sub_block_name, sub_block in basic_block.named_children():\n",
    "#                 if sub_block_name == \"downsample\":\n",
    "#                     torch.ao.quantization.fuse_modules(sub_block,[[\"0\",\"1\"]], inplace=True)\n",
    "# print(fused_model)\n",
    "fused_model = copy.deepcopy(model)\n",
    "fused_model.fuse_model()\n",
    "# Equal Test\n",
    "print(f\"Equal Test between origin and fused\")\n",
    "print(model_eq_check(model,fused_model,device=cpu_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "    print(\"calibrating ...\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 18.92it/s]\n",
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization acc : 90.06 %\n",
      "calibrating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:10<00:00,  7.83it/s]\n",
      "/home/seunmul/.conda/envs/torch/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1135: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "100%|██████████| 79/79 [00:04<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post int8_model acc :85.65 %\n",
      "<class 'models.mobilenetv2.QuantizableMobileNetV2'>\n"
     ]
    }
   ],
   "source": [
    "from models.mobilenetv2 import quat_mobilenet_v2\n",
    "new = False\n",
    "if new:\n",
    "    quat_model = quat_mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1,activation_layer=nn.ReLU)\n",
    "    print(quat_model)\n",
    "    quat_model.classifier.append(nn.Dropout(0.2))\n",
    "    quat_model.classifier.append(nn.Linear(1000, 10))\n",
    "    quat_model.fuse_model()\n",
    "else:\n",
    "    pre_model = copy.deepcopy(fused_model)\n",
    "    _,acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "    print(f\"Before quantization acc : {acc:.2f} %\")\n",
    "    pre_model.eval()\n",
    "    pre_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    torch.quantization.prepare(pre_model,inplace=True)\n",
    "    calibrate_model(pre_model, Test_loader)\n",
    "    pre_model = torch.quantization.convert(pre_model,inplace=True)\n",
    "    torch.jit.save(torch.jit.script(pre_model),\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "    pre_model = torch.jit.load(\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "    _,int8_acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "    print(f\"post int8_model acc :{int8_acc:.2f} %\")\n",
    "    \n",
    "    quat_model = fused_model\n",
    "# qconfig(\"fbgemm\") 은 server 용 \"qnnpack\"은 mobile용 [\"fbgemm\", \"x86\", \"qnnpack\", \"onednn\"]\n",
    "\n",
    "# QAT를 하기위해 quantization 모델 준비\n",
    "quat_model.train()\n",
    "quat_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quat_model = torch.quantization.prepare_qat(quat_model)\n",
    "print(type(quat_model))\n",
    "\n",
    "# print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',quat_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:08<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Acc : 90.06 acc\n",
      "Before Training\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.03 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:50<00:00,  7.70it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------0----------\n",
      "Train 0.6539 Loss, 94.07 Acc\n",
      "Validation 0.7404 Loss, 89.93 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------1----------\n",
      "Train 0.6514 Loss, 94.11 Acc\n",
      "Validation 0.7387 Loss, 90.03 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.94it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2----------\n",
      "Train 0.6492 Loss, 94.16 Acc\n",
      "Validation 0.7379 Loss, 90.11 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.93it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------3----------\n",
      "Train 0.6496 Loss, 94.15 Acc\n",
      "Validation 0.7385 Loss, 90.11 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.90it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------4----------\n",
      "Train 0.6481 Loss, 94.33 Acc\n",
      "Validation 0.7382 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------5----------\n",
      "Train 0.6473 Loss, 94.32 Acc\n",
      "Validation 0.7375 Loss, 90.19 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.02it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------6----------\n",
      "Train 0.6467 Loss, 94.40 Acc\n",
      "Validation 0.7382 Loss, 90.13 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------7----------\n",
      "Train 0.6456 Loss, 94.42 Acc\n",
      "Validation 0.7370 Loss, 90.19 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------8----------\n",
      "Train 0.6463 Loss, 94.31 Acc\n",
      "Validation 0.7370 Loss, 90.14 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------9----------\n",
      "Train 0.6459 Loss, 94.42 Acc\n",
      "Validation 0.7378 Loss, 90.08 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  7.99it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------10----------\n",
      "Train 0.6445 Loss, 94.48 Acc\n",
      "Validation 0.7379 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------11----------\n",
      "Train 0.6450 Loss, 94.32 Acc\n",
      "Validation 0.7373 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.92it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------12----------\n",
      "Train 0.6437 Loss, 94.40 Acc\n",
      "Validation 0.7357 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------13----------\n",
      "Train 0.6420 Loss, 94.55 Acc\n",
      "Validation 0.7363 Loss, 90.28 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------14----------\n",
      "Train 0.6418 Loss, 94.53 Acc\n",
      "Validation 0.7369 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.85it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------15----------\n",
      "Train 0.6427 Loss, 94.39 Acc\n",
      "Validation 0.7365 Loss, 90.33 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.98it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------16----------\n",
      "Train 0.6423 Loss, 94.50 Acc\n",
      "Validation 0.7359 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.89it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------17----------\n",
      "Train 0.6424 Loss, 94.56 Acc\n",
      "Validation 0.7355 Loss, 90.34 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.04it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------18----------\n",
      "Train 0.6417 Loss, 94.48 Acc\n",
      "Validation 0.7352 Loss, 90.25 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.00it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------19----------\n",
      "Train 0.6415 Loss, 94.55 Acc\n",
      "Validation 0.7367 Loss, 90.32 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.01it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------20----------\n",
      "Train 0.6410 Loss, 94.57 Acc\n",
      "Validation 0.7367 Loss, 90.27 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  7.99it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------21----------\n",
      "Train 0.6407 Loss, 94.46 Acc\n",
      "Validation 0.7366 Loss, 90.16 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.93it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------22----------\n",
      "Train 0.6386 Loss, 94.70 Acc\n",
      "Validation 0.7354 Loss, 90.23 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.04it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------23----------\n",
      "Train 0.6408 Loss, 94.48 Acc\n",
      "Validation 0.7362 Loss, 90.27 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.95it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------24----------\n",
      "Train 0.6400 Loss, 94.66 Acc\n",
      "Validation 0.7362 Loss, 90.22 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.03it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------25----------\n",
      "Train 0.6406 Loss, 94.58 Acc\n",
      "Validation 0.7354 Loss, 90.26 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:48<00:00,  8.00it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------26----------\n",
      "Train 0.6407 Loss, 94.47 Acc\n",
      "Validation 0.7352 Loss, 90.30 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.98it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------27----------\n",
      "Train 0.6408 Loss, 94.65 Acc\n",
      "Validation 0.7350 Loss, 90.31 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.97it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------28----------\n",
      "Train 0.6400 Loss, 94.66 Acc\n",
      "Validation 0.7348 Loss, 90.28 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:49<00:00,  7.91it/s]\n",
      "100%|██████████| 79/79 [00:06<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------29----------\n",
      "Train 0.6367 Loss, 94.84 Acc\n",
      "Validation 0.7349 Loss, 90.33 Acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qat_need = True\n",
    "if qat_need:\n",
    "    optimizer = torch.optim.SGD(quat_model.parameters(), lr=1e-5, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,90], gamma=0.5)\n",
    "\n",
    "    first_loss, first_acc = Evaluating(model=quat_model,test_loader=Test_loader,device=cpu_device,criterion=nn.CrossEntropyLoss())\n",
    "    print(f\"Before Acc : {first_acc:.2f} acc\")\n",
    "    quat_model = Training(quat_model,train_loader=Train_loader,test_loader=Test_loader,\n",
    "    device=gpu_device,optimizer=optimizer,scheduler=scheduler,epochs=30,model_name=\"QAT_mobilenetv2_cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:03<00:00, 24.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8_model acc : 88.47 %\n"
     ]
    }
   ],
   "source": [
    "# QAT가 적용된 floating point 모델을 quantized int model로 변환\n",
    "# quat_model.load_state_dict(torch.load(\"./models/QAT_mobilenetv2_cifar10.pt\"))\n",
    "quat_model.to('cpu')\n",
    "int8_model = torch.ao.quantization.convert(quat_model)\n",
    "int8_model.eval()\n",
    "_,int8_acc = Evaluating(int8_model,Test_loader,cpu_device)\n",
    "print(f\"int8_model acc : {int8_acc:.2f} %\")\n",
    "torch.jit.save(torch.jit.script(int8_model),\"./models/QAT_mobilenetv2_cifar10_jit.pt\")\n",
    "int8_model = torch.jit.load(\"./models/QAT_mobilenetv2_cifar10_jit.pt\",map_location=cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 15.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit int8_model acc : 88.47 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_,int8_acc = Evaluating(int8_model,Test_loader,cpu_device)\n",
    "print(f\"jit int8_model acc : {int8_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post int8_model acc :85.65 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_model = torch.jit.load(\"./models/Q_mobilenetv2_cifar10_jit.pt\")\n",
    "_,int8_acc = Evaluating(pre_model,Test_loader,cpu_device)\n",
    "print(f\"post int8_model acc :{int8_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c3f2c56578ca93d34255952c36601b421b6ecc37a0d9982e6b92a246c07692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
