{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 1.12.1\n",
      "torchvision = 0.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "print(f\"torch = {torch.__version__}\")\n",
    "print(f\"torchvision = {torchvision.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         QuantStub-1            [-1, 3, 32, 32]               0\n",
      "            Conv2d-2           [-1, 32, 16, 16]             864\n",
      "       BatchNorm2d-3           [-1, 32, 16, 16]              64\n",
      "             ReLU6-4           [-1, 32, 16, 16]               0\n",
      "            Conv2d-5           [-1, 32, 16, 16]             288\n",
      "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
      "             ReLU6-7           [-1, 32, 16, 16]               0\n",
      "            Conv2d-8           [-1, 16, 16, 16]             512\n",
      "       BatchNorm2d-9           [-1, 16, 16, 16]              32\n",
      "QuantizableInvertedResidual-10           [-1, 16, 16, 16]               0\n",
      "           Conv2d-11           [-1, 64, 16, 16]           1,024\n",
      "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
      "            ReLU6-13           [-1, 64, 16, 16]               0\n",
      "           Conv2d-14             [-1, 64, 8, 8]             576\n",
      "      BatchNorm2d-15             [-1, 64, 8, 8]             128\n",
      "            ReLU6-16             [-1, 64, 8, 8]               0\n",
      "           Conv2d-17             [-1, 32, 8, 8]           2,048\n",
      "      BatchNorm2d-18             [-1, 32, 8, 8]              64\n",
      "QuantizableInvertedResidual-19             [-1, 32, 8, 8]               0\n",
      "           Conv2d-20            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-21            [-1, 128, 8, 8]             256\n",
      "            ReLU6-22            [-1, 128, 8, 8]               0\n",
      "           Conv2d-23            [-1, 128, 8, 8]           1,152\n",
      "      BatchNorm2d-24            [-1, 128, 8, 8]             256\n",
      "            ReLU6-25            [-1, 128, 8, 8]               0\n",
      "           Conv2d-26             [-1, 32, 8, 8]           4,096\n",
      "      BatchNorm2d-27             [-1, 32, 8, 8]              64\n",
      "         Identity-28             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-29             [-1, 32, 8, 8]               0\n",
      "           Conv2d-30            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-31            [-1, 128, 8, 8]             256\n",
      "            ReLU6-32            [-1, 128, 8, 8]               0\n",
      "           Conv2d-33            [-1, 128, 8, 8]           1,152\n",
      "      BatchNorm2d-34            [-1, 128, 8, 8]             256\n",
      "            ReLU6-35            [-1, 128, 8, 8]               0\n",
      "           Conv2d-36             [-1, 32, 8, 8]           4,096\n",
      "      BatchNorm2d-37             [-1, 32, 8, 8]              64\n",
      "         Identity-38             [-1, 32, 8, 8]               0\n",
      "QuantizableInvertedResidual-39             [-1, 32, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 8, 8]           4,096\n",
      "      BatchNorm2d-41            [-1, 128, 8, 8]             256\n",
      "            ReLU6-42            [-1, 128, 8, 8]               0\n",
      "           Conv2d-43            [-1, 128, 4, 4]           1,152\n",
      "      BatchNorm2d-44            [-1, 128, 4, 4]             256\n",
      "            ReLU6-45            [-1, 128, 4, 4]               0\n",
      "           Conv2d-46             [-1, 64, 4, 4]           8,192\n",
      "      BatchNorm2d-47             [-1, 64, 4, 4]             128\n",
      "QuantizableInvertedResidual-48             [-1, 64, 4, 4]               0\n",
      "           Conv2d-49            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-50            [-1, 256, 4, 4]             512\n",
      "            ReLU6-51            [-1, 256, 4, 4]               0\n",
      "           Conv2d-52            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-53            [-1, 256, 4, 4]             512\n",
      "            ReLU6-54            [-1, 256, 4, 4]               0\n",
      "           Conv2d-55             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-56             [-1, 64, 4, 4]             128\n",
      "         Identity-57             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-58             [-1, 64, 4, 4]               0\n",
      "           Conv2d-59            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-60            [-1, 256, 4, 4]             512\n",
      "            ReLU6-61            [-1, 256, 4, 4]               0\n",
      "           Conv2d-62            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-63            [-1, 256, 4, 4]             512\n",
      "            ReLU6-64            [-1, 256, 4, 4]               0\n",
      "           Conv2d-65             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-66             [-1, 64, 4, 4]             128\n",
      "         Identity-67             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-68             [-1, 64, 4, 4]               0\n",
      "           Conv2d-69            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-70            [-1, 256, 4, 4]             512\n",
      "            ReLU6-71            [-1, 256, 4, 4]               0\n",
      "           Conv2d-72            [-1, 256, 4, 4]           2,304\n",
      "      BatchNorm2d-73            [-1, 256, 4, 4]             512\n",
      "            ReLU6-74            [-1, 256, 4, 4]               0\n",
      "           Conv2d-75             [-1, 64, 4, 4]          16,384\n",
      "      BatchNorm2d-76             [-1, 64, 4, 4]             128\n",
      "         Identity-77             [-1, 64, 4, 4]               0\n",
      "QuantizableInvertedResidual-78             [-1, 64, 4, 4]               0\n",
      "           Conv2d-79            [-1, 256, 4, 4]          16,384\n",
      "      BatchNorm2d-80            [-1, 256, 4, 4]             512\n",
      "            ReLU6-81            [-1, 256, 4, 4]               0\n",
      "           Conv2d-82            [-1, 256, 2, 2]           2,304\n",
      "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
      "            ReLU6-84            [-1, 256, 2, 2]               0\n",
      "           Conv2d-85            [-1, 128, 2, 2]          32,768\n",
      "      BatchNorm2d-86            [-1, 128, 2, 2]             256\n",
      "QuantizableInvertedResidual-87            [-1, 128, 2, 2]               0\n",
      "           Conv2d-88            [-1, 512, 2, 2]          65,536\n",
      "      BatchNorm2d-89            [-1, 512, 2, 2]           1,024\n",
      "            ReLU6-90            [-1, 512, 2, 2]               0\n",
      "           Conv2d-91            [-1, 512, 2, 2]           4,608\n",
      "      BatchNorm2d-92            [-1, 512, 2, 2]           1,024\n",
      "            ReLU6-93            [-1, 512, 2, 2]               0\n",
      "           Conv2d-94            [-1, 128, 2, 2]          65,536\n",
      "      BatchNorm2d-95            [-1, 128, 2, 2]             256\n",
      "         Identity-96            [-1, 128, 2, 2]               0\n",
      "QuantizableInvertedResidual-97            [-1, 128, 2, 2]               0\n",
      "           Conv2d-98            [-1, 512, 2, 2]          65,536\n",
      "      BatchNorm2d-99            [-1, 512, 2, 2]           1,024\n",
      "           ReLU6-100            [-1, 512, 2, 2]               0\n",
      "          Conv2d-101            [-1, 512, 2, 2]           4,608\n",
      "     BatchNorm2d-102            [-1, 512, 2, 2]           1,024\n",
      "           ReLU6-103            [-1, 512, 2, 2]               0\n",
      "          Conv2d-104            [-1, 128, 2, 2]          65,536\n",
      "     BatchNorm2d-105            [-1, 128, 2, 2]             256\n",
      "        Identity-106            [-1, 128, 2, 2]               0\n",
      "QuantizableInvertedResidual-107            [-1, 128, 2, 2]               0\n",
      "          Conv2d-108           [-1, 1280, 2, 2]         163,840\n",
      "     BatchNorm2d-109           [-1, 1280, 2, 2]           2,560\n",
      "           ReLU6-110           [-1, 1280, 2, 2]               0\n",
      "         Dropout-111                 [-1, 1280]               0\n",
      "          Linear-112                   [-1, 10]          12,810\n",
      "     DeQuantStub-113                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 656,298\n",
      "Trainable params: 656,298\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.27\n",
      "Params size (MB): 2.50\n",
      "Estimated Total Size (MB): 5.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models import quat_mobilenet_v2\n",
    "tiny_model = quat_mobilenet_v2(cifar10=True)\n",
    "tiny_model.load_state_dict(torch.load(\"../models/weights/tiny_mobilenetv2_cifar.pt\"))\n",
    "summary(tiny_model,(3,32,32),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new _axis_list torch.Size([32, 3, 3, 3]), [0, 1, 2, 3]\n",
      "permete torch.Size([32, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in tiny_model.named_parameters():\n",
    "    x_dim = param.size()\n",
    "    new_axis_list = [i for i in range(len(x_dim))]\n",
    "    print(f\"new _axis_list      {x_dim}, {new_axis_list}\")\n",
    "    y = param.permute(new_axis_list)\n",
    "    print(f\"permete     {y.size()}\")\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c3f2c56578ca93d34255952c36601b421b6ecc37a0d9982e6b92a246c07692"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
