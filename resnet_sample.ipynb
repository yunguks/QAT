{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch = 1.13.0a0+08820cb\n",
      "torchvision = 0.14.0a0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def memory_check():\n",
    "    print(f\"  Allocated: {round(torch.cuda.memory_allocated()/1024**3,2)} GB\")\n",
    "    print(f\"  Cached:    {round(torch.cuda.memory_reserved()/1024**3,2)} GB\\n\")\n",
    "\n",
    "print(f\"torch = {torch.__version__}\")\n",
    "print(f\"torchvision = {torchvision.__version__}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ImageNet(validation 6G) Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "if not os.path.exists(\"./data/ImageNet/meta.bin\"):\n",
    "    print(\"Meta data download\")\n",
    "    wget.download(url=\"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\", out=\"./data/ImageNet\")\n",
    "# if not os.path.exists(\"./data/ImageNet/ILSVRC2012_devkit_t3.tar.gz\"):\n",
    "#     print(\"Toolkit t3 Download\")\n",
    "#     toolkit_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t3.tar.gz\"\n",
    "#     wget.download(url= toolkit_url,out=\"./data/ImageNet\")\n",
    "if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_val.tar\"):\n",
    "    print(\"Download val data\")\n",
    "    val_url  = 'https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar'\n",
    "    wget.download(url=val_url, out=\"./data/ImageNet\")\n",
    "\n",
    "# if not os.path.exists(\"./data/ImageNet/ILSVRC2012_img_train_t3.tar\"):\n",
    "#     print(\"Download train t3 data\")\n",
    "#     train_url = \"https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train_t3.tar\"\n",
    "#     wget.download(url=train_url,out=\"./data/ImageNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_loader(split_num = [0.07,0.03,0.9]):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    print(os.getcwd())\n",
    "    dataset = torchvision.datasets.ImageNet(root=\"./data/ImageNet\",split=\"val\", transform = train_transform)\n",
    "    Train_dataset, Test_dataset,_ = torch.utils.data.random_split(dataset, split_num)\n",
    "    print(f\"Train data set = {len(Train_dataset)}, Test = {len(Test_dataset)}\")\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(Train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(Test_dataset)\n",
    "\n",
    "    Train_loader = torch.utils.data.DataLoader(dataset=Train_dataset, batch_size=2, sampler = train_sampler)\n",
    "    Test_loader = torch.utils.data.DataLoader(dataset=Test_dataset, batch_size =2, sampler = test_sampler)\n",
    "    return Train_loader, Test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "             ReLU6-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "             ReLU6-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 16, 112, 112]             512\n",
      "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
      "  InvertedResidual-9         [-1, 16, 112, 112]               0\n",
      "           Conv2d-10         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 112, 112]             192\n",
      "            ReLU6-12         [-1, 96, 112, 112]               0\n",
      "           Conv2d-13           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-14           [-1, 96, 56, 56]             192\n",
      "            ReLU6-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-18           [-1, 24, 56, 56]               0\n",
      "           Conv2d-19          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 56, 56]             288\n",
      "            ReLU6-21          [-1, 144, 56, 56]               0\n",
      "           Conv2d-22          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 56, 56]             288\n",
      "            ReLU6-24          [-1, 144, 56, 56]               0\n",
      "           Conv2d-25           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 56, 56]              48\n",
      " InvertedResidual-27           [-1, 24, 56, 56]               0\n",
      "           Conv2d-28          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 56, 56]             288\n",
      "            ReLU6-30          [-1, 144, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "         Dropout-157                 [-1, 1280]               0\n",
      "          Linear-158                 [-1, 1000]       1,281,000\n",
      "================================================================\n",
      "Total params: 3,504,872\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.87\n",
      "Params size (MB): 13.37\n",
      "Estimated Total Size (MB): 166.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "from torchsummary import summary\n",
    "summary(model,(3,224,224), device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Fuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluating(model, test_loader, device, criterion):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(iter(test_loader)):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels).detach()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader)\n",
    "    eval_accuracy = running_corrects / len(test_loader)\n",
    "\n",
    "    return eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(model, train_loader, test_loader, device, optimizer, scheduler, epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Before Training\")\n",
    "    torch.cuda.memory_reserved()\n",
    "    memory_check()\n",
    "    # Training\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in tqdm(iter(train_loader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    " \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # statistics\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            del preds\n",
    "        scheduler.step()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = running_corrects / len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        val_loss, val_acc = Evaluating(model,test_loader,device=device,criterion=criterion)\n",
    "        print(f\"--------{epoch}----------\")\n",
    "        print(f\"Train {train_loss:.4f} Loss, {train_accuracy:.2f} Acc\")\n",
    "        print(f\"Validation {val_loss:.4f} Loss, {val_acc:.2f} Acc\")\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer fusion Check\n",
    "conv, bn, relu를 하나의 layer로 만들어 각각의 layer를 읽어오는 연산을 줄이는 과정   \n",
    "folding과는 다른 경량화 기법   \n",
    "Fusion 된 layer는 identity로 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eq_check(model1, model2, device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,256,256)):\n",
    "\n",
    "    model1.to(device)\n",
    "    model2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model1(x).detach().cpu().numpy()\n",
    "        y2 = model2(x).detach().cpu().numpy()\n",
    "        # 배열이 허용 오차범위 abs(a - b) <= (atol + rtol * absolute(b)) 이내면 True\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test fail\")\n",
    "            return False\n",
    "    print(\"Two models equal\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_test(model, device, input_size = (1,3,256,256),num_tests=100,):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(num_tests):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "    aver_time = total_time / num_tests\n",
    "    return total_time, aver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBnReLUModel(\n",
      "  (conv): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "ConvBnReLUModel(\n",
      "  (conv): ConvReLU2d(\n",
      "    (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn): Identity()\n",
      "  (relu): Identity()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "-- Equal Test --\n",
      "Two models equal\n",
      "-- Infer Time Test --\n",
      "origin model infer time 0.053s\n",
      "fusion model infer time 0.041s\n"
     ]
    }
   ],
   "source": [
    "class ConvBnReLUModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBnReLUModel,self).__init__()\n",
    "        self.conv = nn.Conv2d(3,5,3,bias=True).to(dtype=torch.float)\n",
    "        self.bn = nn.BatchNorm2d(5).to(dtype=torch.float)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "model = ConvBnReLUModel().to(device=torch.device(\"cpu:0\"))\n",
    "model.eval()\n",
    "print(model)\n",
    "# for p in model.named_parameters():\n",
    "#     print(p)\n",
    "#     print()\n",
    "# \"fbgemm\" for server , \"qnnpack\" for mobile \n",
    "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# # torch.quantization.fuse_module or myModel.fuse_model()\n",
    "fuse_model = torch.ao.quantization.fuse_modules(model,[['conv','bn','relu']], inplace=False)\n",
    "# fuse_model = model.fuse_model()\n",
    "print(fuse_model)\n",
    "\n",
    "print(f\"-- Equal Test --\")\n",
    "model_eq_check(model, fuse_model, device=torch.device(\"cpu:0\"))\n",
    "\n",
    "\n",
    "print(f\"-- Infer Time Test --\")\n",
    "ori_cpu_time,_ = time_test(model,torch.device(\"cpu\"))\n",
    "fus_cpu_time,_ = time_test(fuse_model,torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"origin model infer time {ori_cpu_time:.3f}s\")\n",
    "print(f\"fusion model infer time {fus_cpu_time:.3f}s\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(QuantizationModel, self).__init__()\n",
    "        \n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        self.fu_model = model\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Convert tensor from float32 to int8\n",
    "        x = self.quant(x)\n",
    "        x = self.fu_model(x)\n",
    "        # Convert tensor from int8 to float32\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/seunmul/QAT\n",
      "Train data set = 3500, Test = 1500\n"
     ]
    }
   ],
   "source": [
    "# gpu,cpu device 선언\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# model 가져오기\n",
    "model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "# from models.Resnet import resnet18\n",
    "# model = resnet18(weights=True)\n",
    "# model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# 모델 저장하고 불러오기 \n",
    "# torch.save(model.state_dict(), \"./models/resnet50.pth\")\n",
    "# # Load a pretrained model.\n",
    "# model.load_state_dict(torch.load(\"./models/resnet50.pth\", map_location=gpu_device)) \n",
    "\n",
    "# Move the model to CPU since static quantization does not support CUDA currently.\n",
    "# ImageNet Data \n",
    "Train_loader, Test_loader = Make_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "1 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "2 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "3 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "4 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "5 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "6 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "7 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "8 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "9 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "10 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "11 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "12 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "13 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "14 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "15 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "16 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "17 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "18 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "19 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "20 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n",
      "21 data size = torch.Size([2, 3, 224, 224]), label size = torch.Size([2])\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(Train_loader):\n",
    "\n",
    "    img = data[0].to(gpu_device)\n",
    "    label = data[1].to(gpu_device)\n",
    "    print(f\"{i} data size = {img.size()}, label size = {label.size()}\")\n",
    "    memory_check()\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n",
      "Equal Test between origin and fused\n",
      "Model equivalence test fail\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델을 CPU상태로 두고 eval로 layer fusion\n",
    "model.eval()\n",
    "\n",
    "# Layer fusion\n",
    "fused_model = torch.quantization.fuse_modules(model,[[\"conv1\",\"bn1\",\"relu\"]])\n",
    "\n",
    "for module_name, module in fused_model.named_children():\n",
    "    if \"layer\" in module_name:\n",
    "        # basic_block 의 conv1, bn1, relu, conv2, bn2 를 fusion\n",
    "        for basic_block_name, basic_block in module.named_children():\n",
    "            torch.ao.quantization.fuse_modules(basic_block,[[\"conv1\",\"bn1\",\"relu\"],[\"conv2\",\"bn2\"]],inplace=True)\n",
    "            # basic_block안의 downsampling block의 Conv2d Batchnorm2D fusion\n",
    "            for sub_block_name, sub_block in basic_block.named_children():\n",
    "                if sub_block_name == \"downsample\":\n",
    "                    torch.ao.quantization.fuse_modules(sub_block,[[\"0\",\"1\"]], inplace=True)\n",
    "print(fused_model)\n",
    "\n",
    "# Equal Test\n",
    "print(f\"Equal Test between origin and fused\")\n",
    "print(model_eq_check(model,fused_model,device=cpu_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): ConvReLU2d(\n",
      "    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
      "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    (activation_post_process): HistogramObserver()\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "          (activation_post_process): HistogramObserver()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "          (activation_post_process): HistogramObserver()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "          (activation_post_process): HistogramObserver()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        1024, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "          (activation_post_process): HistogramObserver()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): ConvReLU2d(\n",
      "        2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (conv2): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): Conv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (bn3): BatchNorm2d(\n",
      "        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (activation_post_process): HistogramObserver()\n",
      "      )\n",
      "      (relu): Identity()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(\n",
      "    in_features=2048, out_features=1000, bias=True\n",
      "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "    (activation_post_process): HistogramObserver()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "quat_model = QuantizationModel(model=fused_model)\n",
    "quat_model.train()\n",
    "# qconfig(\"fbgemm\") 은 server 용 \"qnnpack\"은 mobile용 [\"fbgemm\", \"x86\", \"qnnpack\", \"onednn\"]\n",
    "quat_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "print(quat_model.qconfig)\n",
    "\n",
    "# QAT를 하기위해 quantization 모델 준비\n",
    "quat_model = torch.quantization.prepare_qat(quat_model)\n",
    "print(quat_model.fu_model)\n",
    "\n",
    "# print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',quat_model.features[1].conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training\n",
      "  Allocated: 0.0 GB\n",
      "  Cached:    0.02 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1750 [00:04<17:59,  1.62it/s]  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 9.54 GiB (GPU 0; 23.69 GiB total capacity; 14.62 GiB already allocated; 7.05 GiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/seunmul/QAT/sample.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(\"Before Training\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# val_loss, val_acc = Evaluating(quat_model,Test_loader,device=gpu_device,criterion=nn.CrossEntropyLoss())\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(f\"Before Loss : {val_loss:.4f}, Before Acc : {val_acc:.1f}\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m quat_model \u001b[39m=\u001b[39m Training(quat_model,train_loader\u001b[39m=\u001b[39;49mTrain_loader,test_loader\u001b[39m=\u001b[39;49mTest_loader,device\u001b[39m=\u001b[39;49mgpu_device,optimizer\u001b[39m=\u001b[39;49moptimizer,scheduler\u001b[39m=\u001b[39;49mscheduler,epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# QAT가 적용된 floating point 모델을 quantized int model로 변환\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m quat_model\u001b[39m.\u001b[39mto(cpu_device)\n",
      "\u001b[1;32m/seunmul/QAT/sample.ipynb Cell 21\u001b[0m in \u001b[0;36mTraining\u001b[0;34m(model, train_loader, test_loader, device, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/seunmul/QAT/sample.ipynb Cell 21\u001b[0m in \u001b[0;36mQuantizationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Convert tensor from float32 to int8\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfu_model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Convert tensor from int8 to float32\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f79756e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7365756e6d756c5f74656d70227d7d/seunmul/QAT/sample.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdequant(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1151\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1152\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1153\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1154\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/quantize.py:101\u001b[0m, in \u001b[0;36m_observer_forward_hook\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_observer_forward_hook\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, output):\n\u001b[1;32m     99\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Forward hook that calls observer on the output\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation_post_process(output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1112\u001b[0m, in \u001b[0;36mHistogramObserver.forward\u001b[0;34m(self, x_orig)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     combined_histogram \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram\n\u001b[1;32m   1111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     combined_histogram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_combine_histograms(\n\u001b[1;32m   1113\u001b[0m         combined_histogram,\n\u001b[1;32m   1114\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistogram,\n\u001b[1;32m   1115\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsample_rate,\n\u001b[1;32m   1116\u001b[0m         downsample_rate,\n\u001b[1;32m   1117\u001b[0m         start_idx,\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbins,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram\u001b[39m.\u001b[39mdetach_()\u001b[39m.\u001b[39mresize_(combined_histogram\u001b[39m.\u001b[39mshape)\n\u001b[1;32m   1122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram\u001b[39m.\u001b[39mcopy_(combined_histogram)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1058\u001b[0m, in \u001b[0;36mHistogramObserver._combine_histograms\u001b[0;34m(self, orig_hist, new_hist, upsample_rate, downsample_rate, start_idx, Nbins)\u001b[0m\n\u001b[1;32m   1053\u001b[0m histogram_with_output_range[\n\u001b[1;32m   1054\u001b[0m     start_idx : Nbins \u001b[39m*\u001b[39m upsample_rate \u001b[39m+\u001b[39m start_idx\n\u001b[1;32m   1055\u001b[0m ] \u001b[39m=\u001b[39m upsampled_histogram\n\u001b[1;32m   1056\u001b[0m \u001b[39m# Compute integral histogram, double precision is needed to ensure\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39m# that there are no overflows\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m integral_histogram \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcumsum(\n\u001b[1;32m   1059\u001b[0m     histogram_with_output_range, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdouble\n\u001b[1;32m   1060\u001b[0m )[downsample_rate \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m :: downsample_rate]\n\u001b[1;32m   1061\u001b[0m \u001b[39m# Finally perform interpolation\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m shifted_integral_histogram \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((Nbins), device\u001b[39m=\u001b[39morig_hist\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 9.54 GiB (GPU 0; 23.69 GiB total capacity; 14.62 GiB already allocated; 7.05 GiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(quat_model.parameters(), lr=1e-03, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "# print(\"Before Training\")\n",
    "# val_loss, val_acc = Evaluating(quat_model,Test_loader,device=gpu_device,criterion=nn.CrossEntropyLoss())\n",
    "# print(f\"Before Loss : {val_loss:.4f}, Before Acc : {val_acc:.1f}\")\n",
    "quat_model = Training(quat_model,train_loader=Train_loader,test_loader=Test_loader,device=gpu_device,optimizer=optimizer,scheduler=scheduler,epochs=10)\n",
    "\n",
    "# QAT가 적용된 floating point 모델을 quantized int model로 변환\n",
    "quat_model.to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
